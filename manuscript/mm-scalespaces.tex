\chapter{Linear Scale Space Theory} \label{sec:scale-space-theory}
    
    Although the ideas presented above require differentiation of continuous
    surfaces, our image is in fact composed of discrete pixels. That is, our previous discussions have been in terms of an image as the continuous surface in \cref{def:image_as_surface},
    rather than the more realistic discrete pixel matrix as in \cref{def:image_as_pixel_matrix}.
    The present section seeks to address this disconnect. Divided differences can serve as an analogue of differentiation in discrete contexts, but our "derivative" and any use of it is then completely dependent on the bias of our limited sampling of the ``true'' 3D surface. Our main goal is to counter against some of the bias of our particular sampling. In particular, we wish to not over-represent structures that are clear at our resolution without giving appropriate weight to larger structures as well. Ideally, we would like statements we make about higher order phenomena to be stable enough at least that we would arrive at the same conclusions from analyzing a higher resolution image of the same surface.
    Koenderink \cite{Koenderink} argued that "any image can be embedded in a one-parameter family of derived images (with resolution as the parameter) in essentially only one unique way" given a few of the so-called scale space axioms. He (and others) showed that a small set of intuitive axioms imply that any such family of images $\{K_\sigma\}$ must satisfy the heat equation
    
    \begin{equation} \label{eq:heat-eq-for-family}
    \begin{dcases}
    \Delta K(x,y;\sigma) = \frac{\partial K}{\partial \sigma}(x,y;\sigma)
    \;\text{for}\;\; \sigma \ge 0  \\
    K(x,y, 0) = u_0(x,y)
    \end{dcases}
    \end{equation}
    
    where $u_0: \R^2 \to \R$ is the original image (viewed as a continuous surface), $\sigma$ is the resolution parameter, and $K: \R^2 \to \R $ for each fixed $\sigma$.
    
    
    This result is intuitive and desirable--we would anticipate a lower-resolution version of our original image to represent a diffusion or ``blurring'' of the initial scales information. Much work has been done to formalize this approach \cite{GSST-book}. This has resulted in various formulations of a minimal set of axioms from which all other desirable properties of the scale space can be derived, culminating eventually in the necessity and sufficiency of \cref{eq:heat-eq-for-family} itself. We will refrain from such an exhaustive development in the present text, but rather list some desirable properties and outline the approach.
	    
    \subsection{Properties/Axioms of Linear Scale Space Theory}
	 To make matters manageable, we require the one-parameter family to be generated by an operator on the original image:
	 
	\begin{equation}
		\left\{\ K(x,y;\sigma) = T_\sigma u_0
		\ \lvert \ 
		\sigma \ge 0
		\; ,\; K(x,y,;0) = u_0
		\ \right\} 
	\end{equation}
    
	The following axioms are then requirements on what sort of operation $T_\sigma$ should be.
		
    \begin{axiom}[Linear-shift and Rotational Invariance]
    	\label{axiom:linear-shift-and-rotation}
    We require that no position in the original signal is favored.  This is intuitive, as our operation should apply to any image fairly, regardless of where content is found in the image, and cropping or rotating our initial image should not affect resolution of the content.
\end{axiom}
   
     \begin{axiom}[Semigroup property] \label{axiom:semigroup}
   	The semigroup property is simply that transforming
   	the original image by some resolution $\sigma$ should
   	have the same overall effect of two successive
   	transformations $\sigma_1$ and $\sigma_2$, i.e.
   	
   	\begin{equation}	
   	T_{\sigma} u = T_{\sigma_1 + \sigma_2} u
   	\end{equation}
   \end{axiom}
 
    \begin{axiom}[Continuity of Scale Parameter]
    	\label{axiom:continuity} There is no reason for the scale parameter to be discrete; we may alter the resolution with whatever precision we desire. That is, we take the resolution
    parameter $\sigma$ to be any nonzero real number. Moreover, we require that the operator behaves continuously with respect to the scale parameter.
        \end{axiom}
    
%    As $\sigma \downarrow 0$, we would like to end up the initial conditions (i.e. so that $T_0 u_0 = u_0$). 	This requirement, given \cref{axiom:semigroup} and an argument from functional analysis (see \cite{hille1957functional}) implies that there exists an infinitesimal generator $A$
%	which is the limit case of our desired operator $T$; that is
%	\begin{equation} \label{eq:infinitesimal-generator}
%	A u_0 = \lim_{\sigma \downarrow 0} \frac{T_\sigma u_0 - u_0}{\sigma}
%	\end{equation}.
	
	 
	
    
  
%     \subsubsection{Linearity of generator}
%        ``Linearity implies that all-scale space properties valid for the original signal will transfer to its derivatives. Hence, there is no commitment to certain aspects of image structure, such as the zero-order representation, or its first- or second-order derivatives.''
   
	The following requirement has great implication, and is also
	very successful in encoding our intuitive sense
	of resolution.
   \begin{axiom}[Causality Condition] \label{axiom:causality}
    The causality condition is the one
    that, as resolution decreases, no finer detail is
    introduced into the image. That is, as the scale
    increases, there will be no creation of local extrema
    that did not exist at a smaller scale.
    \end{axiom}
    To make this more precise, if $K(x_0,y_0 ; \sigma_0)$ (that is, a point $(x_0,y_0)$ at some particular
    resolution $\sigma_0$) is a local maximum at that resolution,
    then an increase in scale cannot make this maximum more prominent, i.e.
    \begin{equation}
    \left\{\begin{aligned}
    \nabla K(x_0,y_0; \sigma_0) &= 0 \\
    \Delta K(x_0,y_0;\sigma_0) &< 0
    \end{aligned}\right.
	\quad \implies \quad
	K(x_0,y_0;\sigma_1) \le K(x_0,y_0;\sigma_0)
	\; \forall\; \sigma_1 \ge \sigma_0
    \end{equation}
    
    Similarly, if $K(x_0,y_0;\sigma_0)$ is a local minimum (with respect to space), then an increase in scale cannot make such a valley more profound, i.e.
   \begin{equation}
   \left\{\begin{aligned}
   \nabla K(x_0,y_0; \sigma_0) &= 0 \\
	\Delta K(x_0,y_0;\sigma_0) &> 0
	\end{aligned}\right.
	\quad \implies \quad
	K(x_0,y_0;\sigma_1) \ge K(x_0,y_0;\sigma_0)
	\; \forall\; \sigma_1 \ge \sigma_0
	\end{equation}
    
    This encodes our intuition that no image feature should be sharpened by a descrease in resolution. The only result is a (non-strictly) monotonic blurring of the image as scale parameter $\sigma$ tends to infinity.

    \subsection{Sufficiency of the Gaussian Kernel}
    
    Although we will omit it here, the above requirements are actually sufficient in proving not only that the operator $T_\sigma$ is a convolution, but that the heat equation described in \cref{eq:heat-eq-for-family} must hold. This has been shown in various ways, both by Koenderink \cite{Koenderink}, Babaud \cite{babaud}, as well as Lindeberg in \cite{GSST-book}. Of course, once \cref{eq:heat-eq-for-family} has been established, it is straightforward to show that
    
    \begin{equation}
       K(x,y;\sigma) = T_{\sigma} u_0 = G_\sigma \star u_0
       	\quad \textrm{where}\quad
       	G_\sigma := \frac{1}{2\pi \sigma^2} e^{\left(-\abs{x}^2 / (2\sigma^2)\right)}
        \end{equation}
    
    is a solution.  That is, the family can be generated by convolution with a Gaussian kernel. Lindeberg and others furthered this by arguing that f$T_\sigma$'s continuity as $\sigma$ approaches zero ultimately implies that convolution by a Gaussian the \textit{unique} operator that generates this scale space. 
%   To this, show that:
%    \begin{itemize}
%    \item a kernel satisfying the above axioms must satisfy the heat equation
%  	\item the gaussian kernel satisfies that.
%    \item gaussian kernel is the only kernel that works.
%    \end{itemize}
%    
%    That is,
%    
%    
%	We can show that this solution solves the heat equation.
%	Given $u_0$ as a continuous image (unscaled),
%	we construct PDE with this as a boundary condition.
%    
%    \begin{equation} \label{heat-eq}
%    u: \R^2 \supset \Omega \to \R \; \textrm{with} \; u(\bm{x},t) : \;
%    \begin{cases}
%    \frac{\partial u}{\partial t} (\bm{x}, t) = \Delta u(\bm{x},t) & ,\; t \ge 0 \\
%    u(\bm{x},0) = u_0(\bm{x}) 
%    \end{cases}
%    \end{equation}
%    
%    We show that
%    \begin{equation}
%    u(\bm{x},t) = \left(G_{\sqrt{2t}} \star u_0 \right)(\bm{x})
%    \end{equation}
%    solves \cref{heat-eq}.
%
%    First, we need a quick lemma regarding differentiation a continuous convolution.
%    \begin{lemma} \label{dconvolution}
%    	Derivative of a convolution is the way that it is (obviously rewrite this).
%    \end{lemma}
%    \begin{proof}
%    	For a single variable,
%    	\begin{align}
%    	\frac{\partial}{\partial \alpha} \left[ f(\alpha) \star g(\alpha) \right]
%    	&= \frac{\partial}{\partial \alpha} \left[ 
%    	\int f(t) g(\alpha - t) dt \right] \\
%    	&=  \int f(t) \frac{\partial}{\partial \alpha}\left[ g(\alpha - t)  \right] dt \\
%    	&=  \int f(t) \left(\frac{\partial g}{\partial \alpha}\right) g(\alpha - t) dt \\
%    	&=  f(\alpha) \star g'(\alpha)
%    	\end{align}
%    	By symmetry of convolution we can also conclude 
%    	\[\frac{\partial}{\partial \alpha} \left[ f(\alpha) \star g(\alpha) \right]
%    	= f'(\alpha) \star g(\alpha)
%    	\]
%    	
%    	If $f$ and $g$ are twice differentiable, we can compound this result to show a similar statement holds for second derivatives, and then, given the additivity of convolution,
%    	we may conclude
%    	\begin{equation}
%    	\Delta \left(f \star g \right) = \Delta(f) \star g = f \star \Delta(g) 
%    	\end{equation} 
%    \end{proof}
%    \begin{theorem}
%    	$u(\bm{x},t) = \left(G_{\sqrt{2t}} \star u_0 \right)(\bm{x})$ solves the heat equation.
%    \end{theorem}
%    \begin{proof}
%    	We focus on the particular kernel
%    	\[
%    	G_{\sqrt{2t}} = \frac{1}{4\pi t} e^{\left(-\abs{x}^2 / (4t)\right)}
%    	\]
%    	
%    	
%    	Then
%    	\begin{align}
%    	\frac{\partial u}{\partial t} (\bm{x}, t)
%    	&= \frac{\partial}{\partial t} \left(G_{\sqrt{2t}}(\bm{x},t) \star u_0(\bm{x})\right)  \\
%    	&= \frac{\partial}{\partial t} \left(G_{\sqrt{2t}}(\bm{x},t)\right) \star u_0(\bm{x})  \\
%    	&= \frac{\partial}{\partial t} \left(
%    	\frac{1}{4\pi t} e^{\left(-\abs{x}^2 / (4t)\right)} \right) \star u_0(\bm{x}) \\
%    	&= \left[
%    	-\frac{1}{4\pi t^2} e^{\left(-\abs{x}^2 / (4t)\right)}
%    	+ \frac{1}{4\pi t}\left(\frac{-\abs{x}^2}{4t^2}\right) e^{-\abs{x}^2 / (4t)}
%    	\right] \star u_0(\bm{x}) \\
%    	&= -\frac{1}{4t^2} \left( e^{\left(-\abs{x}^2 / (4t)\right)} 
%    	+ \abs{\bm{x}}^2 G_{\sqrt{2t}}(\bm{x},t)
%    	\right) \star u_0(\bm{x})
%    	\end{align}
%    	and from the previous lemma,
%    	\[
%    	\Delta u(\bm{x}, t) = \Delta\left( G_{\sqrt{2t}} \star u_0(\bm{x})\right)
%    	= \Delta\left( G_{\sqrt{2t}} \right)\star u_0(\bm{x})
%    	\]
%    	
%    	We explicitly calculate the Laplacian of $G_{\sigma}(x,y) = A \exp(-\frac{x^2 + y^2}{2\sigma^2})$ as follows:
%    	
%    	\begin{align*}
%    	\frac{\partial}{\partial x} G_{\sigma}(x,y)
%    	&= A \left( \frac{-2x}{2\sigma^2}\right) \exp\left(-\frac{x^2 + y^2}{2\sigma^2}\right) \\
%    	\implies \frac{\partial^2}{\partial^2 x} G_{\sigma}(x,y)
%    	&= A \cdot \frac{\partial}{\partial x}
%    	\left[ - \frac{x}{\sigma^2} \exp\left(-\frac{x^2 + y^2}{2\sigma^2}\right) \right] \\
%    	&= A \left[ - \frac{1}{\sigma^2} \exp\left(-\frac{x^2 + y^2}{2\sigma^2}\right) 
%    	+ \frac{x}{\sigma^2} \cdot \frac{2x}{2\sigma^2} \exp\left(-\frac{x^2 + y^2}{2\sigma^2}\right) \right] \\
%    	&= A \exp\left(-\frac{x^2 + y^2}{2\sigma^2}\right)
%    	\left[ - \frac{1}{\sigma^2} + \frac{x^2}{\sigma^4} \right] \\
%    	&= \frac{1}{\sigma^2} G_\sigma(x,y)  \left[ \frac{x^2}{\sigma^2} - 1\right]
%    	\end{align*}
%    	
%    	By symmetry of argument we also may conclude
%    	\[
%    	\frac{\partial^2}{\partial y^2} G_{\sigma}(x,y) = \frac{1}{\sigma^2} G_\sigma(x,y)  \left[ \frac{y^2}{\sigma^2} - 1\right]
%    	\]
%    	
%    	and so
%    	
%    	\begin{equation}
%    	\Delta G_\sigma(x,y) =
%    	\frac{\partial^2}{\partial x^2} \left(G_{\sigma}\right)
%    	+ \frac{\partial^2}{\partial y^2} \left(G_{\sigma}\right)
%    	= \frac{1}{\sigma^2} G_\sigma(x,y) \left[ \frac{x^2 + y^2}{\sigma^2} - 2\right] 
%    	\end{equation}
%    	Then, given \cref{dconvolution}, we conclude
%    	\begin{equation}
%    	\Delta \left[ G_\sigma(x,y) \star u_0(x,y) \right] 
%    	= \left(\frac{1}{\sigma^2} G_\sigma(x,y) \left[ \frac{x^2 + y^2}{\sigma^2} - 2\right]\right) \star u_0(x,y)
%    	\end{equation}
%    	
%    	For particular choices of $\sigma(t) = \sqrt{2t}$ and $A = \frac{1}{4\pi t}$,
%    	we see 
%    	\begin{align}
%    	\Delta \left[ G_{\sqrt{2t}}(x,y) \star u_0(x,y) \right] 
%    	&= \left(\frac{1}{2t} G_{\sqrt{2t}}(x,y) \left[ \frac{x^2 + y^2}{2t} - 2\right]\right) \star u_0(x,y) \\
%    	&= \left(G_{\sqrt{2t}}(x,y) \left[ \frac{x^2 + y^2}{4t^2} - \frac{1}{t}\right]\right) \star u_0(x,y)
%    	\end{align}
%    	We then calculate the time derivative,
%    	using our particular choice of $\sigma(t) = \sqrt{2t}$ and $A = \frac{1}{4\pi t}$ as:
%    	
%    	\begin{align}
%    	\frac{\partial}{\partial t} \left[ G_{\sigma(t)}(x,y) \star u_0(x,y) \right]
%    	&= \frac{\partial}{\partial t} \left[ G_{\sigma(t)}(x,y) \right] \star u_0(x,y) \\
%    	&= \frac{\partial}{\partial t} \left[ G_{\sqrt{2t}}(x,y)\right] \star u_0(x,y) \\
%    	&= \frac{\partial}{\partial t} \left[
%    	\frac{1}{4\pi t} \exp\left(-\frac{x^2 + y^2}{4t}\right) \right] \star u_0(x,y) \\
%    	&= \left[ -\frac{1}{4\pi t^2} \exp\left(-\frac{x^2 + y^2}{4t}\right) + 
%    	\frac{1}{4\pi t}\left( \frac{x^2 + y^2}{4t^2} \exp\left(-\frac{x^2 + y^2}{4t}\right)\right)
%    	\right] \star u_0(x,y) \\
%    	&= \left(G_{\sqrt{2t}}(x,y) \left[ \frac{x^2 + y^2}{4t^2} -\frac{1}{t}\right]\right) \star u_0(x,y)
%    	\end{align}
%    	
%    	Combining these results, we find that
%    	\begin{equation}
%    	\frac{\partial}{\partial t} \left[ G_{\sqrt{2t}} \star u_0 \right]
%    	= \Delta \left[ G_{\sqrt{2t}} \star u_0 \right] 
%    	\end{equation}
%    	
%    	as desired. \end{proof}
%    
    
    
    \subsection{Scale Spaces over Discrete Structures} \label{subsec:discrete-scale-space}
    
    The above developments from scale space axioms have (since their first appearance)
    been recast in terms of discrete structures (rather than continuous surfaces) as in \cite{lindeberg-discrete}. However, we've chosen to present the above in their original continuous surface for clarity of argument. The discrete case is not much different--
    we still have the same axioms, and it can be shown that the family of scaled images
    must simply satisfy a discrete version of the heat equation.
    However, viewing our actual image
    \cref{def:image_as_pixel_matrix} as a sample of a continuous
    surface \cref{def:image_as_surface},
    we might  expect our convolution by the Gaussian to ``commute'' with our supposed sampling of the continuous signal,
    or even that we could simply convolve our discrete signal with a discretely sampled Gaussian kernel. The latter in fact, seems to be an often implemented interpretation of scale space theory.
    
    To be clear, the ``sampled'' 1D Gaussian Kernel we have in mind might be given by:
   
    \begin{defn}[Sampled Gaussian Kernel and Generated Family]
    	\[
    	g(n ; \sigma) = \frac{1}{2\pi \sigma} e^{-n^2 / 2\sigma} \;,\quad -\infty < n < \infty
    	\]
    	\end{defn}
    and the  resulting (1D) convolution would be given by
    \[
	    K(x,\sigma) = \sum_{n=-\infty}^{\infty} g(n;\sigma) f(x-n)
	    \quad \textrm{for} \quad x \in \Z, \; \sigma > 0
    \]
    In \cite{lindeberg-discrete} and in particular \cite{lindeberg1988-discreteconstruction}, Lindeberg demonstrated that the sampled Gaussian kernel violates not only semigroup property (\cref{axiom:semigroup}), but--much less forgivably--the causality property (\cref{axiom:semigroup}). There is absolutely no guarantee that convolution with a sampled Gaussian kernel will not create ``spurious'' structures as resolution increases.
    
    
    Fortunately, Lindeberg was immediately able to remedy this by providing a discrete analogue of the Gaussian kernel, which does satisfy \cref{axiom:causality} and \cref{axiom:semigroup}:
    
    \begin{defn}[Discrete Gaussian Kernel]
    	The discrete Gaussian kernel, which can be shown to be a suitable generator
    	for scale space, is given by
    	\begin{equation}
    	T(n;\sigma) = e^{-\alpha \sigma} I_n(\alpha \sigma) ,\quad\,
    	 I_n(\sigma) = I_{-n}(\sigma) = (-1)^n J_n(i\sigma) 
    	 \quad n \ge 0 , \sigma,\alpha > 0
    	 \end{equation}
    \end{defn}
    where $I_n$ are the modified Bessel functions of integer order based on the
    ordinary Bessel functions $J_n$, i.e.
    \[
    I_n(x) = \sum_{m=0}^{\infty} \frac{1}{m! (m+n)!}
	    	\left(\frac{x}{2}\right)^{2m+n} \;,\quad n \ge 0
    \]
    where we have taken the liberty of simplifying the typical definition \cite{abramowitz-stegun} (which involves the gamma function), since we only desire
    Bessel functions of integer order. The parameter $\alpha$ above is simply an
    optional scaling parameter which is simply set to $1$ hereforth.
    
    The derived family of 1D signals is then given by
\begin{equation} \label{eq:derived-family-from-discrete}
        K(x,\sigma) = \sum_{n=-\infty}^{\infty} T(n;t) f(x-n)
        \quad \textrm{for} \quad x \in \Z, \; t > 0
        \end{equation}
    
    The compatibility of scale space theory and derivatives on discrete structures and     extension to two dimensions was also demonstrated by Lindeberg in \cite{lindeberg-discrete-derivative} and \cite{lindeberg1998feature}. In particular, we may take derivatives of the convolutions of our discrete images
    using, say, a local central difference.
    Lastly, the 2D version of the family given in \cref{eq:derived-family-from-discrete} can be obtained by independent convolution of its dimensions (i.e. it is separable). We will make these
    ideas explicit in \cref{ch:implementations}.