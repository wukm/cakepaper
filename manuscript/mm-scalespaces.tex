\chapter{Linear Scale Space Theory} \label{sec:scale-space-theory}
    
    Although the ideas presented above require differentiation of continuous
    surfaces, our image is in fact a discrete pixel. That is, our previous discussions have been in terms of an image as the continuous surface in \cref{def:image_as_surface},
    rather than the more realistic discrete pixel matrix as in \cref{def:image_as_pixel_matrix}.
    The present section seeks to address this disconnect. Divided differences can serve as an analogue of differentiation in discrete contexts, but our "derivative" and any use of it is then completely dependent on the bias of our limited sampling of the ``true'' 3D surface. Our main goal is to counter against some of the bias of our particular sampling. In particular, we wish to not over-represent structures that
    are clear at our resolution without giving appropriate weight to larger structures as well. Ideally, we would like statements we make about higher order phenomena to be stable enough at least that we would arrive at the same conclusions from analyzing a higher resolution image of the same surface.
    Koenderink \cite{Koenderink} argued that "any image can be embedded in a one-parameter family of derived images (with resolution as the parameter) in essentially only one unique way" given a few of the so-called scale space axioms. He (and others) showed that a small set of intuitive axioms imply that any such family of images $\{K_\sigma\}$ must satisfy the heat equation
    
    \begin{equation} \label{eq:heat-eq-for-family}
    \begin{dcases}
    \Delta K(x,y;\sigma) = \frac{\partial K}{\partial \sigma}(x,y;\sigma)
    \;\text{for}\; \sigma \ge 0  \\
    K(x,y, 0) = u_0(x,y)
    \end{dcases}
    \end{equation}
    
    where $u_0: \R^2 \to \R$ is the original image (viewed as a continuous surface), $\sigma$ is the resolution parameter, and $K: \R^2 \to \R $ for each fixed $\sigma$.
    
    
    Much work has been done to formalize this approach \cite{GSST-book}. There is a long list of desired properties that this formalization has sought to reduce to an axiomatic subset. We will simply introduce a few of the desired properties.
	    
    \subsection{Properties/Axioms of Linear Scale Space Theory}
	 To make matters manageable, we require the one-parameter family to be generated by an operator on the original image:
	 
	\begin{equation}
		\left\{\ K(x,y;\sigma) = T_\sigma u_0
		\ \lvert \ 
		\sigma \ge 0
		\; ,\; K(x,y,;0) = u_0
		\ \right\} 
	\end{equation}
    
	The following axioms are then requirements on what sort of operation $T_\sigma$ should be.
		
    \begin{axiom}[Linear-shift and Rotational Invariance]
    	\label{axiom:linear-shift-and-rotation}
    Linear-shift (or translation) invariance means that no position in the original signal is favored.  This is intuitive, as our operation should apply to any image fairly, regardless of where content is found in the image. Similarly, there should be not be favoritism toward any particular orientation of content within the image.
\end{axiom}
    
    \begin{axiom}[Continuity of Scale Parameter]
    	\label{axiom:continuity} There is no reason for the scale parameter to be discrete; we may alter the resolution with whatever
    precision we desire. That is, we take the resolution
    parameter $\sigma$ to be a nonzero real number
    (as opposed to an integer). Moreover, we require that the operator behaves continuously with respect to the scale parameter.
        \end{axiom}
    What happens as $\sigma \downarrow 0$ is not immediately clear though. 	An argument from functional analysis (see \cite{hille1957functional}) implies that there is a so-called ``infinitesimal generator'' $A$
	which is a limit case of our desired operator $T$; that is
	\begin{equation} \label{eq:infinitesimal-generator}
	A u_0 = \lim_{\sigma \downarrow 0} \frac{T_\sigma u_0 - u_0}{\sigma}
	\end{equation}
	 
	 and moreover that there is a resultant differential equation
	 concerning the derivative of the family and $A$:
	 
	 \begin{equation}
	 \partial_{\sigma} K(x,y;\sigma)
	 = \lim_{\sigma \downarrow 0}
	 \frac{K(\cdot;\sigma+h) - K(\cdot; \sigma)}{h}
	 = A(T_\sigma u) = A(K(\cdot,\sigma))
	 \end{equation}
	 
	 We shall return to this idea later and more concretely describe $A$ once we actually characterize the generating 
	 operator $T_\sigma$.
    
    \begin{axiom}[Semigroup property] \label{axiom:semigroup}
    The semigroup property is simply that transforming
    the original image by some resolution $\sigma$ should
    have the same overall effect of two successive
    transformations $\sigma_1$ and $\sigma_2$, i.e.
    
    \begin{equation}	
	    T_{\sigma} u = T_{\sigma_1 + \sigma_2} u
    \end{equation}
    \end{axiom}
%     \subsubsection{Linearity of generator}
%        ``Linearity implies that all-scale space properties valid for the original signal will transfer to its derivatives. Hence, there is no commitment to certain aspects of image structure, such as the zero-order representation, or its first- or second-order derivatives.''
   

   \begin{axiom}[Causality Conditon] \label{axiom:causality}
    The following requirement has great implication, and is also
    very successful in encoding our intuitive sense
    of ``resolution''. The causality condition is the one
    that, as resolution decreases, no finer detail is
    introduced into the image. That is, as the scale
    increases, there will be no creation of local extrema
    that did not exist at a smaller scale.
    \end{axiom}
    In other words, if 
    $K(x_0,y_0 ; \sigma_0)$ is a local maximum (at the point $(x_0, y_0)$, at this fixed $\sigma_0$)
    i.e. 
    then an increase in scale can only weaken this peak, i.e.
    \begin{equation}
    \left\{\begin{aligned}
    \nabla K(x_0,y_0; \sigma_0) &= 0 \\
    \Delta K(x_0,y_0;\sigma_0) &< 0
    \end{aligned}\right.
	\quad \implies \quad
	K(x_0,y_0;\sigma_1) \le K(x_0,y_0;\sigma_0)
	\; \forall\; \sigma_1 \ge \sigma_0
    \end{equation}
    
    Similarly, if $K(x_0,y_0;\sigma_0)$ is a local minimum (with respect to space), then an increase in scale cannot make such a valley more profound, i.e.
   \begin{equation}
   \left\{\begin{aligned}
   \nabla K(x_0,y_0; \sigma_0) &= 0 \\
	\Delta K(x_0,y_0;\sigma_0) &> 0
	\end{aligned}\right.
	\quad \implies \quad
	K(x_0,y_0;\sigma_1) \ge K(x_0,y_0;\sigma_0)
	\; \forall\; \sigma_1 \ge \sigma_0
	\end{equation}
    
    This implies that no image feature is sharpened by an decrease and resolution--the only result is a monotonic blurring of the image as scale parameter $\sigma$ tends to infinity.


    \subsection{Uniqueness of the Gaussian Kernel}
    
    The above requirements are actually sufficient in proving not only that the operator $T_\sigma$ is a convolution, but that the heat equation described in \cref{eq:heat-eq-for-family} must hold. This has been shown in various ways, both by Koenderink \cite{Koenderink}, Babaud \cite{babaud}, as well as Lindeberg in \cite{GSST-book}. In fact, it is shown that the Gaussian is the unique convolution kernel that works.    
    
   To this, show that:
    \begin{itemize}
    \item a kernel satisfying the above axioms must satisfy the heat equation
  	\item the gaussian kernel satisfies that.
    \item gaussian kernel is the only kernel that works.
    \end{itemize}
    
    That is,
    \begin{equation}
   K(x,y;\sigma) = T_{\sigma} u_0 = G_\sigma \star u_0
   	\quad \textrm{where}\quad
   	G_\sigma := \frac{1}{2\pi \sigma^2} e^{\left(-\abs{x}^2 / (2\sigma^2)\right)}
    \end{equation}
    
	We can show that this solution solves the heat equation.
	Given $u_0$ as a continuous image (unscaled),
	we construct PDE with this as a boundary condition.
    
    \begin{equation}
    u: \R^2 \supset \Omega \to \R \; \textrm{with} \; u(\bm{x},t) : \;
    \begin{cases}
    \frac{\partial u}{\partial t} (\bm{x}, t) = \Delta u(\bm{x},t) & ,\; t \ge 0 \\
    u(\bm{x},0) = u_0(\bm{x}) 
    \end{cases}
    \end{equation}
    
    We show that
    \begin{equation}
    u(\bm{x},t) = \left(G_{\sqrt{2t}} \star u_0 \right)(\bm{x})
    \end{equation}
    solves (the above tagged equation), where
    \[
    s
    \]
    First, we need a quick lemma regarding differentiation a continuous convolution.
    \begin{lemma} \label{dconvolution}
    	Derivative of a convolution is the way that it is (obviously rewrite this).
    \end{lemma}
    \begin{proof}
    	For a single variable,
    	\begin{align}
    	\frac{\partial}{\partial \alpha} \left[ f(\alpha) \star g(\alpha) \right]
    	&= \frac{\partial}{\partial \alpha} \left[ 
    	\int f(t) g(\alpha - t) dt \right] \\
    	&=  \int f(t) \frac{\partial}{\partial \alpha}\left[ g(\alpha - t)  \right] dt \\
    	&=  \int f(t) \left(\frac{\partial g}{\partial \alpha}\right) g(\alpha - t) dt \\
    	&=  f(\alpha) \star g'(\alpha)
    	\end{align}
    	By symmetry of convolution we can also conclude 
    	\[\frac{\partial}{\partial \alpha} \left[ f(\alpha) \star g(\alpha) \right]
    	= f'(\alpha) \star g(\alpha)
    	\]
    	
    	If $f$ and $g$ are twice differentiable, we can compound this result to show a similar statement holds for second derivatives, and then, given the additivity of convolution,
    	we may conclude
    	\begin{equation}
    	\Delta \left(f \star g \right) = \Delta(f) \star g = f \star \Delta(g) 
    	\end{equation} 
    \end{proof}
    \begin{theorem}
    	$u(\bm{x},t) = \left(G_{\sqrt{2t}} \star u_0 \right)(\bm{x})$ solves the heat equation.
    \end{theorem}
    \begin{proof}
    	We focus on the particular kernel
    	\[
    	G_{\sqrt{2t}} = \frac{1}{4\pi t} e^{\left(-\abs{x}^2 / (4t)\right)}
    	\]
    	
    	
    	Then
    	\begin{align}
    	\frac{\partial u}{\partial t} (\bm{x}, t)
    	&= \frac{\partial}{\partial t} \left(G_{\sqrt{2t}}(\bm{x},t) \star u_0(\bm{x})\right)  \\
    	&= \frac{\partial}{\partial t} \left(G_{\sqrt{2t}}(\bm{x},t)\right) \star u_0(\bm{x})  \\
    	&= \frac{\partial}{\partial t} \left(
    	\frac{1}{4\pi t} e^{\left(-\abs{x}^2 / (4t)\right)} \right) \star u_0(\bm{x}) \\
    	&= \left[
    	-\frac{1}{4\pi t^2} e^{\left(-\abs{x}^2 / (4t)\right)}
    	+ \frac{1}{4\pi t}\left(\frac{-\abs{x}^2}{4t^2}\right) e^{-\abs{x}^2 / (4t)}
    	\right] \star u_0(\bm{x}) \\
    	&= -\frac{1}{4t^2} \left( e^{\left(-\abs{x}^2 / (4t)\right)} 
    	+ \abs{\bm{x}}^2 G_{\sqrt{2t}}(\bm{x},t)
    	\right) \star u_0(\bm{x})
    	\end{align}
    	and from the previous lemma,
    	\[
    	\Delta u(\bm{x}, t) = \Delta\left( G_{\sqrt{2t}} \star u_0(\bm{x})\right)
    	= \Delta\left( G_{\sqrt{2t}} \right)\star u_0(\bm{x})
    	\]
    	
    	We explicitly calculate the Laplacian of $G_{\sigma}(x,y) = A \exp(-\frac{x^2 + y^2}{2\sigma^2})$ as follows:
    	
    	\begin{align*}
    	\frac{\partial}{\partial x} G_{\sigma}(x,y)
    	&= A \left( \frac{-2x}{2\sigma^2}\right) \exp\left(-\frac{x^2 + y^2}{2\sigma^2}\right) \\
    	\implies \frac{\partial^2}{\partial^2 x} G_{\sigma}(x,y)
    	&= A \cdot \frac{\partial}{\partial x}
    	\left[ - \frac{x}{\sigma^2} \exp\left(-\frac{x^2 + y^2}{2\sigma^2}\right) \right] \\
    	&= A \left[ - \frac{1}{\sigma^2} \exp\left(-\frac{x^2 + y^2}{2\sigma^2}\right) 
    	+ \frac{x}{\sigma^2} \cdot \frac{2x}{2\sigma^2} \exp\left(-\frac{x^2 + y^2}{2\sigma^2}\right) \right] \\
    	&= A \exp\left(-\frac{x^2 + y^2}{2\sigma^2}\right)
    	\left[ - \frac{1}{\sigma^2} + \frac{x^2}{\sigma^4} \right] \\
    	&= \frac{1}{\sigma^2} G_\sigma(x,y)  \left[ \frac{x^2}{\sigma^2} - 1\right]
    	\end{align*}
    	
    	By symmetry of argument we also may conclude
    	\[
    	\frac{\partial^2}{\partial y^2} G_{\sigma}(x,y) = \frac{1}{\sigma^2} G_\sigma(x,y)  \left[ \frac{y^2}{\sigma^2} - 1\right]
    	\]
    	
    	and so
    	
    	\begin{equation}
    	\Delta G_\sigma(x,y) =
    	\frac{\partial^2}{\partial x^2} \left(G_{\sigma}\right)
    	+ \frac{\partial^2}{\partial y^2} \left(G_{\sigma}\right)
    	= \frac{1}{\sigma^2} G_\sigma(x,y) \left[ \frac{x^2 + y^2}{\sigma^2} - 2\right] 
    	\end{equation}
    	Then, given \cref{dconvolution}, we conclude
    	\begin{equation}
    	\Delta \left[ G_\sigma(x,y) \star u_0(x,y) \right] 
    	= \left(\frac{1}{\sigma^2} G_\sigma(x,y) \left[ \frac{x^2 + y^2}{\sigma^2} - 2\right]\right) \star u_0(x,y)
    	\end{equation}
    	
    	For particular choices of $\sigma(t) = \sqrt{2t}$ and $A = \frac{1}{4\pi t}$,
    	we see 
    	\begin{align}
    	\Delta \left[ G_{\sqrt{2t}}(x,y) \star u_0(x,y) \right] 
    	&= \left(\frac{1}{2t} G_{\sqrt{2t}}(x,y) \left[ \frac{x^2 + y^2}{2t} - 2\right]\right) \star u_0(x,y) \\
    	&= \left(G_{\sqrt{2t}}(x,y) \left[ \frac{x^2 + y^2}{4t^2} - \frac{1}{t}\right]\right) \star u_0(x,y)
    	\end{align}
    	We then calculate the time derivative,
    	using our particular choice of $\sigma(t) = \sqrt{2t}$ and $A = \frac{1}{4\pi t}$ as:
    	
    	\begin{align}
    	\frac{\partial}{\partial t} \left[ G_{\sigma(t)}(x,y) \star u_0(x,y) \right]
    	&= \frac{\partial}{\partial t} \left[ G_{\sigma(t)}(x,y) \right] \star u_0(x,y) \\
    	&= \frac{\partial}{\partial t} \left[ G_{\sqrt{2t}}(x,y)\right] \star u_0(x,y) \\
    	&= \frac{\partial}{\partial t} \left[
    	\frac{1}{4\pi t} \exp\left(-\frac{x^2 + y^2}{4t}\right) \right] \star u_0(x,y) \\
    	&= \left[ -\frac{1}{4\pi t^2} \exp\left(-\frac{x^2 + y^2}{4t}\right) + 
    	\frac{1}{4\pi t}\left( \frac{x^2 + y^2}{4t^2} \exp\left(-\frac{x^2 + y^2}{4t}\right)\right)
    	\right] \star u_0(x,y) \\
    	&= \left(G_{\sqrt{2t}}(x,y) \left[ \frac{x^2 + y^2}{4t^2} -\frac{1}{t}\right]\right) \star u_0(x,y)
    	\end{align}
    	
    	Combining these results, we find that
    	\begin{equation}
    	\frac{\partial}{\partial t} \left[ G_{\sqrt{2t}} \star u_0 \right]
    	= \Delta \left[ G_{\sqrt{2t}} \star u_0 \right] 
    	\end{equation}
    	
    	as desired. \end{proof}
    
    
    
    \subsection{Scale Spaces over Discrete Structures} \label{subsec:discrete-scale-space}
    
    The above developments from scale space axioms have (since their first appearance)
    been recast in terms of discrete structures (rather than continuous surfaces) as in \cite{lindeberg-discrete}. However, we've chosen to present the above in their original continuous surface for clarity of argument. The discrete case is not much different--
    we still have the same axioms, and it can be shown that the family of scaled images
    must simply satisfy a discrete version of the 
    However, viewing our actual image
    \cref{def:image_as_pixel_matrix} as a sample of a continuous
    surface \cref{def:image_as_surface},
    we might na{\"i}vely expect our convolution by the Gaussian to ``commute'' with our supposed sampling of the continuous signal,
    or even that we could simply convolve our discrete signal with a discretely sampled Gaussian kernel. The latter in fact, seems to be an often implemented interpretation of scale space theory.
    
    To be clear, the ``sampled'' 1D Gaussian Kernel we have in mind might be given by:
   
    \begin{defn}[Sampled Gaussian Kernel and Generated Family]
    	\[
    	g(n ; \sigma) = \frac{1}{2\pi \sigma} e^{-n^2 / 2\sigma} \;,\quad -\infty < n < \infty
    	\]
    	\end{defn}
    and the  resulting (1D) convolution would be given by
    \[
	    K(x,\sigma) = \sum_{n=-\infty}^{\infty} g(n;\sigma) f(x-n)
	    \quad \textrm{for} \quad x \in \Z, \; \sigma > 0
    \]
    The reality of the matter is that a discretely sampled Gaussian is not an appropriate kernel for creating discrete scale space. In \cite{lindeberg-discrete} and in particular
    \cite{lindeberg1988-discreteconstruction}, Lindeberg demonstrated that the sampled Gaussian kernel violates not only semigroup property (\cref{axiom:semigroup}), but--much less forgivably--the causality property (\cref{axiom:semigroup}). There is absolutely no guarantee that convolution with a sampled Gaussian kernel will not create ``spurious'' structures as resolution increases.
    
    
    Fortunately, Lindeberg was immediately able to remedy this by providing a discrete analogue of the Gaussian kernel, which does satisfy \cref{axiom:causality} and \cref{axiom:semigroup}:
    
    \begin{defn}[Discrete Gaussian Kernel]
    	The discrete Gaussian kernel, which can be shown to be a suitable generator
    	for scale space, is given by
    	\begin{equation}
    	T(n;\sigma) = e^{-\alpha \sigma} I_n(\alpha \sigma) ,\quad\,
    	 I_n(\sigma) = I_{-n}(\sigma) = (-1)^n J_n(i\sigma) 
    	 \quad n \ge 0 , \sigma,\alpha > 0
    	 \end{equation}
    \end{defn}
    where $I_n$ are the modified Bessel functions of integer order based on the
    ordinary Bessel functions $J_n$, i.e.
    \[
    I_n(x) = \sum_{m=0}^{\infty} \frac{1}{m! (m+n)!}
	    	\left(\frac{x}{2}\right)^{2m+n} \;,\quad n \ge 0
    \]
    where we have taken the liberty of simplifying the typical definition \cite{abramowitz-stegun} (which involves the gamma function), since we only desire
    Bessel functions of integer order. The parameter $\alpha$ above is simply an
    optional scaling parameter which is simply set to $1$ hereforth.
    
    The derived family of 1D signals is then given by
\begin{equation} \label{eq:derived-family-from-discrete}
        K(x,\sigma) = \sum_{n=-\infty}^{\infty} T(n;t) f(x-n)
        \quad \textrm{for} \quad x \in \Z, \; t > 0
        \end{equation}
    
    The compatibility of scale space theory and derivatives on discrete structures and     extension to two dimensions was also demonstrated by Lindeberg in \cite{lindeberg-discrete-derivative} and \cite{lindeberg1998feature}. In particular, we may take derivatives of the convolutions of our discrete images
    using, say, a central difference.
    Lastly, the 2D version of the family given in \cref{eq:derived-family-from-discrete} can be obtained by independent convolution of its dimensions (i.e. it is separable). We will make these
    ideas explicit in \cref{ch:implementations} and the Appendix.