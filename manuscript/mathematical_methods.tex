\chapter{Mathematical Methods}

\section{Overview of Differential Geometry in Image Processing}
	\subsection{Basics, Definitions}
        \begin{defn}
    	For theoretical purposes, we may view any 2D grayscale image
    	as a continuous function $L: \R^2 \to \R$
        with $L \in \mathcal{C}^2\left(\R^2\right).$
        \end{defn}
        \begin{defn}
        In the context of differential geometry, we wish to refer to its graph
            $f: \R^2 \to \R^3 $ by $ (u,v) \mapsto (u, v, L(u,v))$.
    	\end{defn}
    % put this later instead?
        \begin{defn}
    In situations where we wish to discuss a discrete image, we may refer to
	$L_0 \in \R^{m\times n}$. That is, $L_0$ is a matrix corresponding to the $m$-by-$n$ digital grayscale image. 
    \end{defn}
	
	
	Viewing the surface in $R^3$, we define the Hessian $\Hess$ of the surface $L$
	at a point $(x,y)$ on the surface as the matrix of its second partial derivatives:
	\begin{equation}
\mathcal{H}(x,y) = \begin{bmatrix}
		L_{xx}(x,y) & L_{xy}(x,y) \\
		L_{yx}(x,y) & L_{yy}(x,y)
	\end{bmatrix}
	\end{equation}
	
	At any point $(x,y)$ we denote the two eigenpairs of $\Hess(x,y)$ as
	\begin{equation}
		\Hess u_i = \kappa_i u_i \; , \quad i = 1,2
	\end{equation}
	
	where $\kappa_i$ and $u_i$ are known as the
	\textit{principal curvatures} and \textit{principal directions} of $L(x,y)$, respectively, and we label such that $|\kappa_2| \ge |\kappa_1|$. Notably, $\Hess(x,y)$ is a real, symmetric matrix (since  $L_{xy} = L_{yx}$ and $L$ is a real function)and thus its eigenvalues are real and its eigenvectors are orthonormal to each other, as given by following lemma:
	
        % from burden and faires corollary 9.17
        \begin{lemma}[Principal Axis Theorem?]
		Let $A$ be a real, symmetric matrix. The eigenvalues of $A$ are real and its eigenvectors are orthonormal to each other.
	\end{lemma}
	
	\begin{proof}
		Let $x\ne 0$ so that $Ax = \lambda x$. Then 
		\begin{align*}
		\vnorm{Ax}_2^2 = \inner{Ax}{Ax}  &= (Ax)^{*} Ax \\
						&= x^{*}A^{*}Ax = x^{*}A^T A x = x*A A x \\
						&= x^{*} A \lambda x = \lambda x^{*} A x \\
						&= \lambda x^{*} \lambda x = \lambda^2 x^{*} x = \lambda ^2 \vnorm{x}_2^2
		\end{align*}
		Upon rearrangement, we have
		$\lambda^2 = \frac{\vnorm{Ax}_2^2}{\vnorm{x}_2^2} \ge 0 \implies \lambda $ is real.
		
		To prove that a set of orthonormalizable eigenvectors exists,
		let $A$ be real, symmetric as above and consider the eigenpairs
		$Av_1 = \lambda_1 v_1$, $Av_2 = \lambda_2 v_2$ with $v_1, v_2 \ne 0$.
		\footnote{To simplify notation, we simplify our argument to consider two explicit eigenvectors only, since we're only concerned with the $2\times 2$ matrix $\Hess$
			anyway.}
		
		In the case that $\lambda_1 \ne \lambda_2$, we have
		\begin{align*}
			(\lambda_1 - \lambda_2)v_1^T v_2 &= \lambda_1 v_1^T v_2 - \lambda_2 v_1^T v_2 \\
										  	 &= (\lambda_1 v_1)^T v_2 - v_1^T (\lambda_2 v_2) \\
										  	 &= (Av_1)^T v_2 - v_1^T (Av_2) \\
										  	 &= v_1^T A^T v_2 - v_1^T A v_2 \\
										  	 &= v_1^T A v_2 - v_1^T A v_2 = 0
			\end{align*}
			Since $\lambda_1 \ne \lambda_2$, we conclude that $v_1^T v_2 = 0$.
		
		In the case that $\lambda_1 = \lambda_2 =: \lambda$, we can define
		(as in Gram-Schmidt orthogonalization) $u = v_2 - \frac{v_1^Tv_2}{v_1^Tv_1}v_1$.
		This is an eigenvector for $\lambda=\lambda_2$, as
		\begin{align*}
		Au &= A\left(v_2 - \frac{v_1^Tv_2}{v_1^Tv_1} v_1\right) \\
		   &= A v_2 - \frac{v_1^Tv_2}{v_1^Tv_1} A v_1 \\
		   &= \lambda v_2- \frac{v_1^Tv_2}{v_1^Tv_1} \lambda v_1 \\
		   &= \lambda \left( v_2 - \frac{v_1^Tv_2}{v_1^Tv_1} v_1 \right) = \lambda u
		\end{align*}
		and is perpendicular to $v_1$, since
			\begin{align*}
			v_1^T u &= v_1^T\left(v_2 - \frac{v_1^Tv_2}{v_1^Tv_1}v_1\right) \\
					&= v_1^T v_2 - \left(\frac{v_1^Tv_2}{v_1^Tv_1}\right) v_1^T v_1 \\
					&= v_1^T v_2 - v_1^Tv_2 (1) = 0.
			\end{align*}
	\end{proof}
	
        Thus we see that the two principal directions form an orthonormal frame at each point (x,y) within the continuous image $L(x,y)$.

        \maltese The following is an \textbf{unverified claim} (which might be useful for later):
            The frame varies continuously along paths in $\R^2$
            except at points where $\Hess(x,y)$ is singular.
        To make this explicit:

        \begin{theorem}[Continuity of the leading principal direction]
            Let $\theta: I:= [0,1] \to \R^2$ be a parametrized regular curve in $\R^2$ and
            $H_\theta := \Hess_f  \circ \theta(t)$ be the matrix-valued function
            % use the same notation as the Hessian before and cite the eqn. number from earlier
            (where $\Hess_f$ is the $2\times 2$ Hessian of the smooth surface $f$)
            Let $U:I \to \R^2$ be the implicitly-defined vector valued function s.t.
            $U(t)$ is the leading eigenvector of $H_\theta$
            (and therefore the leading principal direction of $f$). That is,
            \begin{equation}
                H_\theta \; U(t) = \lambda U(t) \quad \textrm{with}\quad \lambda = \rho(H_\theta)
            \end{equation}
            In other words, $\abs{\lambda} \ge \abs{\tilde{\lambda}}$ for any
            $\tilde{\lambda} : H_\theta \; u = \tilde{\lambda} u$ for some $u \ne 0$.
            
            Then, $U(t)$ is continuous in $t$ whenever $H_f(t)$ is non-singular.
            \textit{Note:} Maybe fix this so that the path avoids any nonsingular points?
            $U(t)$ isn't even well-defined at such points anyway.
        \end{theorem}
        \begin{proof}
            First, we show that $U(t)$ is a well-defined function at all points $t$ where
            $H_f(t)$ is non-singular.
        \end{proof}

         \maltese \textbf{TODO}       

\hrulefill

\section{Eigenvectors of the Hessian are principal curvature directions}
	Formally, within the context of differential geometry, we should show a few things:
	\begin{itemize}
		\item definition of curvature of a surface (how it relates to curvature of a regular curve) and showing that it's invariant of which curve is taken--that is, it only depends on a tangent vector to the surface.
		\item normal curvature is given by the second fundamental form
			$\textbf{II}(X,X) = \inner{\mathsf{L}X}{X}$ where $\mathsf{L}$ is the Weingarten map.
		\item $X$ is an extremal value of $\textbf{II}(X,X)$ subject to $\textbf{I}(X,X) = 1 $ iff $X$ is an eigenvector of $L$.
		\item In the case of a surface $f(u,v) = (u,v,h(u,v))$ (i.e. graph of a 2D function, which is the only situation we care about in image processing), the Weingarten map is exactly the Hessian matrix.
		\end{itemize}
        
        \begin{lemma}
            If $A\in R^{n\times n}$ is a symmetric real matrix, $v \in R^n$
            and given the dot product $\inner{\cdot}{\cdot}$,
            we have $\nabla_{v} \inner{v}{Av} = 2Av$.
            In particular, when $A = I$ the identity matrix, we have
            $ \nabla_{v} \inner{v}{v} = 2v$.
        \end{lemma}
        
        \begin{proof}
            The result is uninterestingly obtained by tracking
            each (the `$i$th') component of
            $\nabla_{v} \inner{v}{Av}$:
            
            \begin{align}
            {\Big(\nabla_{v} \inner{v}{Av}\Big)}_{i} =
                    \frac{\partial}{\partial v_i} \Big[
                    \inner{v}{Av}\Big]
                    &=  \frac{\partial}{\partial v_i} \left[
                        \sum_{j=1}^{n} v_j {(Av)}_j\right] \\
                    &=	\frac{\partial}{\partial v_i} \left[
                    \sum_{j=1}^{n} v_j \sum_{k=1}^n a_{jk} v_k \right] \\
                    &= \frac{\partial}{\partial v_i} \left[
                    a_{ii} v_i^2 + v_i \sum_{k\ne i} a_{ik} v_k
				                 + v_i \sum_{j\ne i} a_{ji} v_j
				                 + \sum_{j\ne i} \sum_{k \ne i} v_j a_{jk} v_k \right] \\
				    &=  2 a_{ii} v_i + \sum_{k\ne i} a_{ik} v_k
				    + \sum_{j\ne i} a_{ji} v_j + 0 \\
				    &= 2 a_{ii} v_i + 2 \sum_{k\ne i} a_{ik} v_k
				     = 2 \sum_{k=1}^n a_{ik}v_{k} = 2 {\big(Av\big)}_i \\
				     &\quad \implies \nabla_{v} \inner{v}{Av} = 2Av.
            \end{align}
        \end{proof}
        
        \begin{theorem}[Theorem of Olinde Rodrigues]
        	$X$ minimizes $\textbf{II}(X,X)$ subject to $\textbf{I}(X,X)= 1$
        	iff $X$ is a (normalized) eigenvector of the Weingarten map $\mathsf{L}$.
        	\end{theorem}
        \begin{proof}
        		Recall first the defintion of the first two fundamental forms,
        		$\textbf{II}(X,X) = \inner{\wein{X}}{X}$
        		and $\textbf{I}(X,X) = \inner{X}{X}$.
        		
        		Using the method of Lagrange multipliers, we define the Lagrangian:
        		\begin{equation}
        		\begin{aligned}
        		\mathscr{L}(X; \lambda) &=
	        		\mathbf{II}(X,X) - \lambda\Big(\mathbf{I}(X,X) - 1\Big) \\
	        		&= \inner{\wein X}{X} - \lambda\Big(\inner{X}{X} - 1\Big)
	        	\end{aligned} \end{equation}
	        	
	        	Extremal values occur when
	        	$\nabla_{X,\lambda} \mathscr{L}(X;\lambda) = 0$,
	        	
	        	which becomes the two equations
	        	
	        \begin{equation}
	        \left\{ \begin{aligned}
		        \nabla_X \inner{\wein X}{X} - \lambda \nabla_X \left( \inner{X}{X} - 1 \right) = 0 \\
		        \inner{X}{X} - 1 = 0
	        \end{aligned} \right.
	        \end{equation}
	        
	        The second requirement is simply the constraint that $X$ is normalized.
	        Using the previous lemma, we can simplify the first result as follows:
	        
	        \begin{gather}
	        \nabla_X \inner{\wein X}{X} - \lambda \nabla_X \left( \inner{X}{X} - 1 \right) = 0 \\
	        2\wein X - \lambda \left(2 X \right) = 0 \\
	        \implies LX - \lambda X = 0 \implies LX = \lambda X.
	        \end{gather}
	        Thus the two hypotheses are exactly equivalent when $X$ is normalized. It is also worth remarking that the corresponding eigenvalue $lambda$ is the Lagrangian multiplier itself.
        	\end{proof}
\section{Calculating Derivatives of Discrete Images} 
	Discrete derivatives. \maltese\textbf{TODO:} Develop the following:
	\begin{itemize}
		\item Take derivative with gradient / divided difference.
		\item Derivatives should be taken on Gaussian blur (equivalent to scale space development) [Lindeberg]. That is, you can take the derivative of either the convolved image, or you can take derivatives of the Gaussian itself, \textit{then} convolve.
		\item Pseudocode for \texttt{np.gradient} which is used in calculating Hessian (code below)
					\begin{verbatim}
					gaussian_filtered = fftgauss(image, sigma=sigma)
					Lx, Ly = np.gradient(gaussian_filtered)
					Lxx, Lxy = np.gradient(Lx)
					Lxy, Lyy = np.gradient(Ly)
					\end{verbatim}
				
		\end{itemize}

\hrulefill
\section{The Frangi Filter}


    \subsection{Intro to Hessian-based filters}
	%Basic properties and observations. Why these filters make sense. Basic strengths and weaknesses.

        Hessian-based filters are a family of curvilinear filters that employ the Hessian and its eigenspaces
        to determine regions of significant curvature within an image.
        % as mentioned by Frangi, see Sato [13] and Lorenz [10] in their paper
        Several such filters exist --see Sato [13] and Lorenz[10]. These filters use information about the principal curvatures (eigenvalues of the Hessian) at each point to 
    \subsection{Overview of Frangi vesselness measure}
        The Frangi filter is a widely used [citation needed] Hessian-based filter that relies on the principal curvatures--that is,
        the eigenvalues of $\Hess_\sigma(x,y)$ at some particular scale $\sigma$ at each point (x,y) in the image. 
	\subsection{Implementation Details: Convolution Speedup via FFT}
	As described above, the actual computation of derivatives is achieved via convolution with a gaussian. In practice, this is very slow for large scales. In 
		\subsubsection{TODO}
		\begin{itemize}
			\item find image processing papers that find hessian from FFT / who uses this?
			\item with above: downsides?
			\item SIDE BY SIDE comparison?
		\end{itemize}
		
		\subsubsection{2D Discrete Fourier Transform Convolution Theorem}\footnote{the following was adapted in a large part from DFT: an owner's manual. cite?}
		
		\begin{theorem}[2D DFT Convolution Theorem]
		Given two discrete functions are sequences with the same length\footnote{If they're
			not actually the same length, DIP-GW suggests to make the final length at least
			$P = A+C-1$ and $Q = B+D-1$ in the case that the sizes are $A\times B$ and $C\times D$ for $f(x,y)$ and  $h(x,y)$ respectively. Not sure if that matters.}, that is:
		$f(x,y)$ and $h(x,y)$ for integers $0 < x < M$ and $0 < y < N$, we can take the discrete fourier transform (DFT) of each:
		\begin{align}
		F(u,v) := \mathcal{D}\{f(x,y)\} &=
						\sum_{x=0}^{M-1} \sum_{y=0}^{N-1} f(x,y)
						e^{-2\pi i \left(\frac{ux}{M} + \frac{vy}{N}\right)} \\	
		   H(u,v) := \mathcal{D}\{h(x,y)\} &=
						\sum_{x=0}^{M-1} \sum_{y=0}^{N-1} h(x,y)
						e^{-2\pi i \left(\frac{ux}{M} + \frac{vy}{N}\right)}
		\end{align}
		
		and given the convolution of the two functions
		\begin{equation}
		\left(f \star h\right)(x,y) = \sum_{m=0}^{M-1} \sum_{n=0}^{N-1} f(m,n)h(x-m,y-n)
		\end{equation}
		
		then $\left(f \star h\right)(x,y)$ and $MN\cdot F(u,v)H(u,v)$ are transform pairs, i.e.
		\begin{equation}
		\left(f \star h\right)(x,y) = \mathcal{D}^{-1}\left\{MN\cdot F(u,v)H(u,v)\right\}
		\end{equation}
		\end{theorem}
		The proof follows from the definition of convolution, substituting in the inverse-DFT of $f$ and $h$, and then rearrangement of finite sums.
		\begin{proof}
		\begin{align}
		\left(f \star h\right)(x,y) &= \sum_{m=0}^{M-1} \sum_{n=0}^{N-1} f(m,n)h(x-m,y-n) \\
		&= \sum_{m=0}^{M-1} \sum_{n=0}^{N-1}
		\left(\sum_{p=0}^{M-1} \sum_{q=0}^{N-1} F(p,q)
			e^{2\pi i \left(\frac{mp}{M} + \frac{nq}{N}\right)}\right)
			\left(\sum_{u=0}^{M-1} \sum_{v=0}^{N-1} H(u,v)
			e^{2\pi i \left(\frac{u(x-m)}{M} + \frac{v(y-n)}{N}\right)} \right) \\
		&= \left(\sum_{u=0}^{M-1} \sum_{v=0}^{N-1} H(u,v)
			e^{2\pi i \left(\frac{ux}{M} + \frac{vy}{N}\right)}\right)
			\left(\sum_{p=0}^{M-1} \sum_{q=0}^{N-1} F(p,q)
			\left(\sum_{m=0}^{M-1} e^{2\pi i \left(\frac{m(p-u)}{M}\right)}\right)
			\left(\sum_{n=0}^{N-1} e^{2\pi i \left(\frac{n(q-v)}{N}\right)}\right)\right) \\
			&= \left(\sum_{u=0}^{M-1} \sum_{v=0}^{N-1} H(u,v)
			e^{2\pi i \left(\frac{ux}{M} + \frac{vy}{N}\right)}\right)
			\left(\sum_{p=0}^{M-1} \sum_{q=0}^{N-1} F(p,q)
			\left( M \cdot \hat{\delta}_M(p-u) \right)
			\left( N \cdot \hat{\delta}_M(q-v)\right)\right) \\
			&= \left(\sum_{u=0}^{M-1} \sum_{v=0}^{N-1} H(u,v)
			e^{2\pi i \left(\frac{ux}{M} + \frac{vy}{N}\right)}\right)
			\cdot M N F(u,v) \\
			&=MN \cdot \sum_{u=0}^{M-1} \sum_{v=0}^{N-1} F(u,v) H(u,v)
			e^{2\pi i \left(\frac{ux}{M} + \frac{vy}{N}\right)} \\
			&= MN \cdot \mathcal{D}^{-1}\left\{ FH\right\}
		\end{align}
		
		where
		\begin{equation}
			\hat{\delta}_N (k) = \begin{cases}
				1 & \text{when } k = 0 \mod N \\
				0 & \text{else}
				\end{cases}
		\end{equation}
	\end{proof}
	Above, we make use of the following lemma:
	\begin{lemma}
	Let $j$ and $k$ be integers and let $N$ be a positive integer. Then
	\begin{equation}
		\sum_{n=0}^{N-1} e^{2\pi i\left(\frac{n(j-k)}{N}\right)} =  N \cdot \hat{\delta}_N(j-k)
		\end{equation}
		\end{lemma}
		\begin{proof}
		
		For any particular $n \in 0..N-1$,consider the complex number $e^{2\pi i n(j-k)/N}$. Note first, that this is an $N$-th root of unity, since
		\[
		\left(e^{2\pi i n(j-k)/N}\right)^N = e^{2\pi i n(j-k)} = \left(e^{2\pi i}\right)^{n(j-k)}
		= 1^{n(j-k)} = 1
		\]
	
	Thus, we understand that the complex number $e^{2\pi i n(j-k)/N}$ is a root of $z^N -1 = 0$, which we rewrite as
	\[ z^N -1 \;=\; (z-1)(z^{n-1} + \cdots + z + 1) \;=\; (z-1)\sum_{n=0}^{N-1} z^n .
	\]

We consider two cases: in the case that $j-k$ is a multiple of $N$, we of course have $e^{2\pi i n(j-k)/N} = 1$ for any $n$, and thus
\[
\sum_{n=0}^{N-1} e^{2\pi i\left(\frac{n(j-k)}{N}\right)} = \sum_{n=0}^{N-1} \left(1\right) = N
\].

In the case that $j-k$ is \textit{not} a multiple of $N$, then clearly $\left(e^{2\pi i n(j-k)/N}\right)-1 \ne 0$, and thus it must be that
\[\left(e^{2\pi i n(j-k)/N}\right)^N -1 = 0 \;\implies\; \sum_{n=0}^{N-1} \left(e^{2\pi i n(j-k)/N}\right)^n = 0\]
	
	Combining these two cases gives the result of the lemma.
		\end{proof}
		


		
		\subsubsection{FFT}
		
		As noted, the above result applies to the Discrete Fourier Transform. As noted, we actually achieve a convolution speedup using a Fast Fourier Transform (FFT) instead.
		
		\begin{itemize}
			\item Basic theory.
			\item Show speedup and equivalency.
		\end{itemize}
		
		\subsubsection{Odds \& Ends of Fourier Analysis}
		\begin{itemize}
			\item Sampling theory?
			\item Wraparound error?
			\item Any other kinks introduced in going from continuous to discrete.
		\end{itemize}


\hrulefill
\section{Linear Scale Space Theory}
	Koenderink showed that "any image can be embedded in a one-parameter family of derived images (with resolution as the parameter) in essentially only one unique way" given a few of the so-called \textit{scale space axioms}. They showed in particular that any such family must satisfy the heat equation
	\begin{equation}
		\Delta K(x,y,\sigma) = K_\sigma (x,y,\sigma) 
		\;\text{for}\; \sigma \ge 0
		\;\text{such that}\; K(x,y, 0) = u_0(x,y).
		\end{equation}
		where $K: \R^3 \to \R $ and  $u_0: \R^2 \to \R: $ is the original image (viewed as a continuous surface) and $\sigma$ is a resolution parameter.
	This can be derived using several assumptions.
	

	
\hrulefill
\section{Morphology}
	Methods of merging multiscale methods.
	
	
\hrulefill
\section{Other Odds and Ends}
	Finding the placental plate. Preprocessing. Could go in next section as well.



