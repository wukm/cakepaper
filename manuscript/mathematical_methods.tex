\chapter{Mathematical Methods}

\section{Overview of Differential Geometry in Image Processing}
	\subsection{Basics, Definitions}
        \begin{defn}
    	For theoretical purposes, we may view any 2D grayscale image
    	as a continuous function $L: \R^2 \to \R$
        with $L \in \mathcal{C}^2\left(\R^2\right).$
        \end{defn}
        \begin{defn}
        In the context of differential geometry, we wish to refer to its graph
            $f: \R^2 \to \R^3 $ by $ (u,v) \mapsto (u, v, L(u,v))$.
    	\end{defn}
    % put this later instead?
        \begin{defn}
    In situations where we wish to discuss a discrete image, we may refer to
	$L_0 \in \R^{m\times n}$. That is, $L_0$ is a matrix corresponding to the $m$-by-$n$ digital grayscale image. 
    \end{defn}
	
	
	Viewing the surface in $R^3$, we define the Hessian $\Hess$ of the surface $L$
	at a point $(x,y)$ on the surface as the matrix of its second partial derivatives:
	\begin{equation}
\mathcal{H}(x,y) = \begin{bmatrix}
		L_{xx}(x,y) & L_{xy}(x,y) \\
		L_{yx}(x,y) & L_{yy}(x,y)
	\end{bmatrix}
	\end{equation}
	
	At any point $(x,y)$ we denote the two eigenpairs of $\Hess(x,y)$ as
	\begin{equation}
		\Hess u_i = \kappa_i u_i \; , \quad i = 1,2
	\end{equation}
	
	where $\kappa_i$ and $u_i$ are known as the
	\textit{principal curvatures} and \textit{principal directions} of $L(x,y)$, respectively, and we label such that $|\kappa_2| \ge |\kappa_1|$. Notably, $\Hess(x,y)$ is a real, symmetric matrix (since  $L_{xy} = L_{yx}$ and $L$ is a real function)and thus its eigenvalues are real and its eigenvectors are orthonormal to each other, as given by following lemma:
	
        % from burden and faires corollary 9.17
        \begin{lemma}[Principal Axis Theorem?]
		Let $A$ be a real, symmetric matrix. The eigenvalues of $A$ are real and its eigenvectors are orthonormal to each other.
	\end{lemma}
	
	\begin{proof}
		Let $x\ne 0$ so that $Ax = \lambda x$. Then 
		\begin{align*}
		\vnorm{Ax}_2^2 = \inner{Ax}{Ax}  &= (Ax)^{*} Ax \\
						&= x^{*}A^{*}Ax = x^{*}A^T A x = x*A A x \\
						&= x^{*} A \lambda x = \lambda x^{*} A x \\
						&= \lambda x^{*} \lambda x = \lambda^2 x^{*} x = \lambda ^2 \vnorm{x}_2^2
		\end{align*}
		Upon rearrangement, we have
		$\lambda^2 = \frac{\vnorm{Ax}_2^2}{\vnorm{x}_2^2} \ge 0 \implies \lambda $ is real.
		
		To prove that a set of orthonormalizable eigenvectors exists,
		let $A$ be real, symmetric as above and consider the eigenpairs
		$Av_1 = \lambda_1 v_1$, $Av_2 = \lambda_2 v_2$ with $v_1, v_2 \ne 0$.
		\footnote{To simplify notation, we simplify our argument to consider two explicit eigenvectors only, since we're only concerned with the $2\times 2$ matrix $\Hess$
			anyway.}
		
		In the case that $\lambda_1 \ne \lambda_2$, we have
		\begin{align*}
			(\lambda_1 - \lambda_2)v_1^T v_2 &= \lambda_1 v_1^T v_2 - \lambda_2 v_1^T v_2 \\
										  	 &= (\lambda_1 v_1)^T v_2 - v_1^T (\lambda_2 v_2) \\
										  	 &= (Av_1)^T v_2 - v_1^T (Av_2) \\
										  	 &= v_1^T A^T v_2 - v_1^T A v_2 \\
										  	 &= v_1^T A v_2 - v_1^T A v_2 = 0
			\end{align*}
			Since $\lambda_1 \ne \lambda_2$, we conclude that $v_1^T v_2 = 0$.
		
		In the case that $\lambda_1 = \lambda_2 =: \lambda$, we can define
		(as in Gram-Schmidt orthogonalization) $u = v_2 - \frac{v_1^Tv_2}{v_1^Tv_1}v_1$.
		This is an eigenvector for $\lambda=\lambda_2$, as
		\begin{align*}
		Au &= A\left(v_2 - \frac{v_1^Tv_2}{v_1^Tv_1} v_1\right) \\
		   &= A v_2 - \frac{v_1^Tv_2}{v_1^Tv_1} A v_1 \\
		   &= \lambda v_2- \frac{v_1^Tv_2}{v_1^Tv_1} \lambda v_1 \\
		   &= \lambda \left( v_2 - \frac{v_1^Tv_2}{v_1^Tv_1} v_1 \right) = \lambda u
		\end{align*}
		and is perpendicular to $v_1$, since
			\begin{align*}
			v_1^T u &= v_1^T\left(v_2 - \frac{v_1^Tv_2}{v_1^Tv_1}v_1\right) \\
					&= v_1^T v_2 - \left(\frac{v_1^Tv_2}{v_1^Tv_1}\right) v_1^T v_1 \\
					&= v_1^T v_2 - v_1^Tv_2 (1) = 0.
			\end{align*}
	\end{proof}
	
        Thus we see that the two principal directions form an orthonormal frame at each point (x,y) within the continuous image $L(x,y)$.

        \maltese The following is an \textbf{unverified claim} (which might be useful for later):
            The frame varies continuously along paths in $\R^2$
            except at points where $\Hess(x,y)$ is singular.
        To make this explicit:

        \begin{theorem}[Continuity of the leading principal direction]
            Let $\theta: I:= [0,1] \to \R^2$ be a parametrized regular curve in $\R^2$ and
            $H_\theta := \Hess_f  \circ \theta(t)$ be the matrix-valued function
            % use the same notation as the Hessian before and cite the eqn. number from earlier
            (where $\Hess_f$ is the $2\times 2$ Hessian of the smooth surface $f$)
            Let $U:I \to \R^2$ be the implicitly-defined vector valued function s.t.
            $U(t)$ is the leading eigenvector of $H_\theta$
            (and therefore the leading principal direction of $f$). That is,
            \begin{equation}
                H_\theta \; U(t) = \lambda U(t) \quad \textrm{with}\quad \lambda = \rho(H_\theta)
            \end{equation}
            In other words, $\abs{\lambda} \ge \abs{\tilde{\lambda}}$ for any
            $\tilde{\lambda} : H_\theta \; u = \tilde{\lambda} u$ for some $u \ne 0$.
            
            Then, $U(t)$ is continuous in $t$ whenever $H_f(t)$ is non-singular.
            \textit{Note:} Maybe fix this so that the path avoids any nonsingular points?
            $U(t)$ isn't even well-defined at such points anyway.
        \end{theorem}
        \begin{proof}
            First, we show that $U(t)$ is a well-defined function at all points $t$ where
            $H_f(t)$ is non-singular.
        \end{proof}

         \maltese \textbf{TODO}       

\hrulefill

\section{Eigenvectors of the Hessian are principal curvature directions}
	Formally, within the context of differential geometry, we should show a few things:
	\begin{itemize}
		\item definition of curvature of a surface (how it relates to curvature of a regular curve) and showing that it's invariant of which curve is taken--that is, it only depends on a tangent vector to the surface.
		\item normal curvature is given by the second fundamental form
			$\textbf{II}(X,X) = \inner{\mathsf{L}X}{X}$ where $\mathsf{L}$ is the Weingarten map.
		\item $X$ is an extremal value of $\textbf{II}(X,X)$ subject to $\textbf{I}(X,X) = 1 $ iff $X$ is an eigenvector of $L$.
		\item In the case of a surface $f(u,v) = (u,v,h(u,v))$ (i.e. graph of a 2D function, which is the only situation we care about in image processing), the Weingarten map is exactly the Hessian matrix.
		\end{itemize}
        
        \subsection{Defining normal curvature of a surface}
        In the context of a regular curve $c: I \to \R^3$ parametrized along some closed interval $I\in \R$
	        (that is, one where $c'(s) = 1 \;\; \forall s \in I$), curvature at a point $s \in I$ is defined simply as the magnitude of the curve's acceleration: $\kappa(s) := \vnorm{c''(s)}$.
	    % get this notation right
	    To extend the notion of curvature of a surface $f$, we can consider at some point
	    $u = (u_1, u_2)$ on the surface and consider a parametrized curve with
	    parametrization $\theta: I \to U \subset \R^2$  
		so that we may speak of $c(u) = c \circ \theta(t)$ freely as the curve $c$ at point $u \in U$ and simultaneously refer to it as $c(t) $ for $ t \in I$.
		Of course, $\mathrm{image}(c) \subset \mathrm{image}(f)$ exactly--that is, the curve is on the surface.
		
		We now present a main result that provides a notion of curvature of a surface.
		\begin{theorem}[Theorem of Meusnier]
			Given a point $u \in U$ and a tangent direction $X \in T_u \R^3$ any curve on the surface $f$ 
			such that $c'(u) = X$ will have the same curvature.
		\end{theorem}
		
		\begin{proof}
		Considering any such curve where $\frac{\partial c}{\partial t}(u=\theta(t)) = X$ where $X \in T_u \R^3$ is a normalized vector tangent to the surface at the point $u$,
		we wish to decompose its acceleration along the two orthogonal vectors $X$ and
		the Gauss map $\nu := \nu(u_1, u_2) =
			\frac{\frac{\partial f}{\partial u_1} \times \frac{\partial f}{\partial u_2}}
			{\vnorm{\frac{\partial f}{\partial u_1} \times \frac{\partial f}{\partial u_2}}}$. (Note that $X$ and $\nu$ are indeed orthogonal,
			as both $\frac{\partial f}{\partial u_i} \in T_u \R^3, \; i=1,2.$) We then have
			
			\begin{equation}
				c'' = \inner{c''}{X} X + \inner{c''}{\nu}\nu
				\end{equation}. 
		
		The first term is zero, since for a regular curve,  % is it? why disregard otherwise
		\[ \inner{c''}{X} = \inner{c''}{c'} = 0 \]
		
		Using chain rule, we can rewrite the second coefficient of (2.4) as % prove chain rule?
		\begin{align}
			\inner{c''}{\nu} &=
			\frac{\partial}{\partial t}\left[ \inner{c'}{\nu} \right]
				- \inner{c'}{\frac{\partial \nu}{\partial t}} \\
				&= \frac{\partial}{\partial t}\left[ \inner{X}{\nu} \right]
				- \inner{c'}{\frac{\partial \nu}{\partial t}} \\
				&= 0 - \inner{X}{\frac{\partial \nu}{\partial t}}
				%&= \inner{X}{\wein X} = \mathbf{II}(X,X)
				\end{align}
		
		Thus, we can express the curvature at this point on our selected curve as
		\[
		%\vnorm{c''} = \mathbf{II}(X,X) \vnorm{\nu} = \mathbf{II}(X,X)
		\vnorm{c''} = - \inner{X}{\frac{\partial \nu}{\partial t}}\vnorm{\nu}
		= - \inner{X}{\frac{\partial \nu}{\partial t}}
		\]
		
		which only depends on the point $u$ and the selected direction $X$.
		\end{proof}
		
		In fact, we refer to this invariant quantity as the normal curvature of the surface.
		
		\begin{defn}
			The normal curvature of a surface at point $u$ in the direction $X$ is given by $\kappa_\nu := - \inner{X}{\frac{\partial \nu}{\partial t}}$.
		\end{defn}
		
		In other contexts, this quantity is referred to as the second fundamental form at the point $u \in U$; that is, $ \mathbf{II}(X,X) := - \inner{X}{\frac{\partial \nu}{\partial t}}$.
		
		Our final goal to is to characterize such normal curvatures and establish,
		at a given point $u \in U$, a method of determining in which directions an extremal
		normal curvature occurs.
		To do so, we shall consider the relationship between the direction $X$ and the normal curvature $\kappa_\nu$ in that direction at some specified $u$. That is, we trace $X \in T_u f $ (which may be expanded in the  orthogonal basis
		$\left\{\frac{\partial f}{\partial u_1} , \frac{\partial f}{\partial u_2}\right\}$)
		back to its associated surface element $f(u)$ and then expand it along the basis $\left\{\frac{\partial \nu}{\partial u_1} , \frac{\partial \nu}{\partial u_2}\right\}$.
		
		Formally, we can write this map as $\mathcal{L} = - d\nu \circ (df)^{-1}$. 

	
	That is,
	$\wein \frac{\partial f}{\partial u_i} = - \frac{\partial \nu}{\partial u_i}.$ where we refer to $\wein$  we can write a matrix representation of the map $\mathcal{L}$ as the change of basis matrix %justify this
	\begin{equation}
	\wein := \begin{pmatrix}
	\inner{\frac{\partial f}{\partial u_1}}{-\frac{\partial \nu}{\partial u_1}}
	& \inner{\frac{\partial f}{\partial u_1}}{-\frac{\partial \nu}{\partial u_2}} \\
	\inner{\frac{\partial f}{\partial u_2}}{-\frac{\partial \nu}{\partial u_1}}
	& \inner{\frac{\partial f}{\partial u_2}}{-\frac{\partial \nu}{\partial u_2}}
	\end{pmatrix}
	\end{equation}
		
		First, we need the following lemma:
        \begin{lemma}
            If $A\in R^{n\times n}$ is a symmetric real matrix, $v \in R^n$
            and given the dot product $\inner{\cdot}{\cdot}$,
            we have $\nabla_{v} \inner{v}{Av} = 2Av$.
            In particular, when $A = I$ the identity matrix, we have
            $ \nabla_{v} \inner{v}{v} = 2v$.
        \end{lemma}
        
        \begin{proof}
            The result is uninterestingly obtained by tracking
            each (the `$i$th') component of
            $\nabla_{v} \inner{v}{Av}$:
            
            \begin{align}
            {\Big(\nabla_{v} \inner{v}{Av}\Big)}_{i} =
                    \frac{\partial}{\partial v_i} \Big[
                    \inner{v}{Av}\Big]
                    &=  \frac{\partial}{\partial v_i} \left[
                        \sum_{j=1}^{n} v_j {(Av)}_j\right] \\
                    &=	\frac{\partial}{\partial v_i} \left[
                    \sum_{j=1}^{n} v_j \sum_{k=1}^n a_{jk} v_k \right] \\
                    &= \frac{\partial}{\partial v_i} \left[
                    a_{ii} v_i^2 + v_i \sum_{k\ne i} a_{ik} v_k
				                 + v_i \sum_{j\ne i} a_{ji} v_j
				                 + \sum_{j\ne i} \sum_{k \ne i} v_j a_{jk} v_k \right] \\
				    &=  2 a_{ii} v_i + \sum_{k\ne i} a_{ik} v_k
				    + \sum_{j\ne i} a_{ji} v_j + 0 \\
				    &= 2 a_{ii} v_i + 2 \sum_{k\ne i} a_{ik} v_k
				     = 2 \sum_{k=1}^n a_{ik}v_{k} = 2 {\big(Av\big)}_i \\
				     &\quad \implies \nabla_{v} \inner{v}{Av} = 2Av.
            \end{align}
        \end{proof}
        
        \begin{theorem}[Theorem of Olinde Rodrigues]
        	Fixing a point $u \in U$, a direction $X \in T_u \R^3 $ minimizes the normal curvature $\kappa_\nu = \textbf{II}(X,X)$ subject to $\textbf{I}(X,X)= 1$
        	iff $X$ is a (normalized) eigenvector of the Weingarten map $\mathsf{L}$.
        	\end{theorem}
        \begin{proof}
        		Recall first the definition of the first two fundamental forms,
        		$\textbf{II}(X,X) = \inner{\wein{X}}{X}$
        		and $\textbf{I}(X,X) = \inner{X}{X}$.
        		
        		Using the method of Lagrange multipliers, we define the Lagrangian:
        		\begin{equation}
        		\begin{aligned}
        		\mathscr{L}(X; \lambda) &=
	        		\mathbf{II}(X,X) - \lambda\Big(\mathbf{I}(X,X) - 1\Big) \\
	        		&= \inner{\wein X}{X} - \lambda\Big(\inner{X}{X} - 1\Big)
	        	\end{aligned} \end{equation}
	        	
	        	Extremal values occur when
	        	$\nabla_{X,\lambda} \mathscr{L}(X;\lambda) = 0$,
	        	which becomes the two equations
	        	
	        \begin{equation}
	        \left\{ \begin{aligned}
		        \nabla_X \inner{\wein X}{X} - \lambda \nabla_X \left( \inner{X}{X} - 1 \right) = 0 \\
		        \inner{X}{X} - 1 = 0
	        \end{aligned} \right.
	        \end{equation}
	        
	        The second requirement is simply the constraint that $X$ is normalized.
	        Using the previous lemma, we can simplify the first result as follows:
	        
	        \begin{gather}
	        \nabla_X \inner{\wein X}{X} - \lambda \nabla_X \left( \inner{X}{X} - 1 \right) = 0 \\
	        2\wein X - \lambda \left(2 X \right) = 0 \\
	        \implies LX - \lambda X = 0 \implies LX = \lambda X.
	        \end{gather}
	        Thus the two hypotheses are exactly equivalent when $X$ is normalized. It is also worth remarking that the corresponding eigenvalue $\lambda$ is the Lagrangian multiplier itself.
        	\end{proof}
        	
        	Our final goal is to determine that in the case of $f(u,v) = (u,v,L(u,v))$, the Weingarten
        	map itself is defined as
        	\begin{defn}[The Weingarten map]
        		We define the Weingarten map $\mathcal{L}$ at point $u \in U$ as mapping tangent vectors (in the basis of $\left\{\frac{\partial f}{\partial u_i}\right\}$) to the (negative) derivatives of normal vectors (in the basis $\left\{- \frac{\partial \nu}{\partial u_i}\right\}$).
        		\end{defn}
        
    
        		
        		
        		We are now ready to show that in the case of a graph surface, this matrix is exactly the Hessian matrix.
        		
        	\begin{theorem}
	        	When $f: U \to \R^3$ is given by $(u,v) \mapsto (u,v,L(u,v))$, the matrix
	        	representation of the Weingarten map is exactly the Hessian matrix given in (2.1). % make this a real tag.
        	\end{theorem}
        	\begin{proof}
        		First, we can (using chain rule) rewrite each component as
        		\[ \inner{\frac{\partial f}{\partial u_i}}{-\frac{\partial \nu}{\partial u_j}}
        		= \inner{\frac{\partial^2 f}{\partial u_i \partial u_j}}{\nu} \]
        		
        		Now, given our particular surface $f$, we can calculate each of these components directly. We have:
        		\begin{equation}
        		\begin{gathered}
        		f_{u} = (1, 0, L_u) , \quad
        		f_{v} = (0, 1, L_v)  \\
        		f_{uu} = (0, 0, L_{uu}) , \quad
        		f_{uv} = (0, 0, L_{uv}) = f_{vu} , \quad
        		f_{vv} = (0, 0, L_{vv})
        		\end{gathered}
        		\end{equation}
        		
        		and we have the unit normal vector (Gauss map)
        		\begin{align}
        		\nu(u_1, u_2) &=
        		\frac{\frac{\partial f}{\partial u_1} \times \frac{\partial f}{\partial u_2}}
        		{\vnorm{\frac{\partial f}{\partial u_1} \times \frac{\partial f}{\partial u_2}}} \\
        		&= \frac{(1, 0, L_u) \times (0, 1, L_v)}{\vnorm{\cdots}}
			 \end{align}    		
        		
        	\end{proof}
\section{Calculating Derivatives of Discrete Images} 
	Discrete derivatives. \maltese\textbf{TODO:} Develop the following:
	\begin{itemize}
		\item Take derivative with gradient / divided difference.
		\item Derivatives should be taken on Gaussian blur (equivalent to scale space development) [Lindeberg]. That is, you can take the derivative of either the convolved image, or you can take derivatives of the Gaussian itself, \textit{then} convolve.
		\item Pseudocode for \texttt{np.gradient} which is used in calculating Hessian (code below)
					\begin{verbatim}
					gaussian_filtered = fftgauss(image, sigma=sigma)
					Lx, Ly = np.gradient(gaussian_filtered)
					Lxx, Lxy = np.gradient(Lx)
					Lxy, Lyy = np.gradient(Ly)
					\end{verbatim}
				
		\end{itemize}

\hrulefill
\section{The Frangi Filter}


    \subsection{Intro to Hessian-based filters}
	%Basic properties and observations. Why these filters make sense. Basic strengths and weaknesses.

        Hessian-based filters are a family of curvilinear filters that employ the Hessian and its eigenspaces
        to determine regions of significant curvature within an image.
        % as mentioned by Frangi, see Sato [13] and Lorenz [10] in their paper
        Several such filters exist --see Sato [13] and Lorenz[10]. These filters use information about the principal curvatures (eigenvalues of the Hessian) at each point to 
    \subsection{Overview of Frangi vesselness measure}
        The Frangi filter is a widely used [citation needed] Hessian-based filter that relies on the principal curvatures--that is,
        the eigenvalues of $\Hess_\sigma(x,y)$ at some particular scale $\sigma$ at each point (x,y) in the image. 
	\subsection{Implementation Details: Convolution Speedup via FFT}
	As described above, the actual computation of derivatives is achieved via convolution with a gaussian. In practice, this is very slow for large scales. In 
		\subsubsection{TODO}
		\begin{itemize}
			\item find image processing papers that find hessian from FFT / who uses this?
			\item with above: downsides?
			\item SIDE BY SIDE comparison?
		\end{itemize}
		
		\subsubsection{2D Discrete Fourier Transform Convolution Theorem}\footnote{the following was adapted in a large part from DFT: an owner's manual. cite?}
		
		\begin{theorem}[2D DFT Convolution Theorem]
		Given two discrete functions are sequences with the same length\footnote{If they're
			not actually the same length, DIP-GW suggests to make the final length at least
			$P = A+C-1$ and $Q = B+D-1$ in the case that the sizes are $A\times B$ and $C\times D$ for $f(x,y)$ and  $h(x,y)$ respectively. Not sure if that matters.}, that is:
		$f(x,y)$ and $h(x,y)$ for integers $0 < x < M$ and $0 < y < N$, we can take the discrete fourier transform (DFT) of each:
		\begin{align}
		F(u,v) := \mathcal{D}\{f(x,y)\} &=
						\sum_{x=0}^{M-1} \sum_{y=0}^{N-1} f(x,y)
						e^{-2\pi i \left(\frac{ux}{M} + \frac{vy}{N}\right)} \\	
		   H(u,v) := \mathcal{D}\{h(x,y)\} &=
						\sum_{x=0}^{M-1} \sum_{y=0}^{N-1} h(x,y)
						e^{-2\pi i \left(\frac{ux}{M} + \frac{vy}{N}\right)}
		\end{align}
		
		and given the convolution of the two functions
		\begin{equation}
		\left(f \star h\right)(x,y) = \sum_{m=0}^{M-1} \sum_{n=0}^{N-1} f(m,n)h(x-m,y-n)
		\end{equation}
		
		then $\left(f \star h\right)(x,y)$ and $MN\cdot F(u,v)H(u,v)$ are transform pairs, i.e.
		\begin{equation}
		\left(f \star h\right)(x,y) = \mathcal{D}^{-1}\left\{MN\cdot F(u,v)H(u,v)\right\}
		\end{equation}
		\end{theorem}
		The proof follows from the definition of convolution, substituting in the inverse-DFT of $f$ and $h$, and then rearrangement of finite sums.
		\begin{proof}
		\begin{align}
		\left(f \star h\right)(x,y) &= \sum_{m=0}^{M-1} \sum_{n=0}^{N-1} f(m,n)h(x-m,y-n) \\
		&= \sum_{m=0}^{M-1} \sum_{n=0}^{N-1}
		\left(\sum_{p=0}^{M-1} \sum_{q=0}^{N-1} F(p,q)
			e^{2\pi i \left(\frac{mp}{M} + \frac{nq}{N}\right)}\right)
			\left(\sum_{u=0}^{M-1} \sum_{v=0}^{N-1} H(u,v)
			e^{2\pi i \left(\frac{u(x-m)}{M} + \frac{v(y-n)}{N}\right)} \right) \\
		&= \left(\sum_{u=0}^{M-1} \sum_{v=0}^{N-1} H(u,v)
			e^{2\pi i \left(\frac{ux}{M} + \frac{vy}{N}\right)}\right)
			\left(\sum_{p=0}^{M-1} \sum_{q=0}^{N-1} F(p,q)
			\left(\sum_{m=0}^{M-1} e^{2\pi i \left(\frac{m(p-u)}{M}\right)}\right)
			\left(\sum_{n=0}^{N-1} e^{2\pi i \left(\frac{n(q-v)}{N}\right)}\right)\right) \\
			&= \left(\sum_{u=0}^{M-1} \sum_{v=0}^{N-1} H(u,v)
			e^{2\pi i \left(\frac{ux}{M} + \frac{vy}{N}\right)}\right)
			\left(\sum_{p=0}^{M-1} \sum_{q=0}^{N-1} F(p,q)
			\left( M \cdot \hat{\delta}_M(p-u) \right)
			\left( N \cdot \hat{\delta}_M(q-v)\right)\right) \\
			&= \left(\sum_{u=0}^{M-1} \sum_{v=0}^{N-1} H(u,v)
			e^{2\pi i \left(\frac{ux}{M} + \frac{vy}{N}\right)}\right)
			\cdot M N F(u,v) \\
			&=MN \cdot \sum_{u=0}^{M-1} \sum_{v=0}^{N-1} F(u,v) H(u,v)
			e^{2\pi i \left(\frac{ux}{M} + \frac{vy}{N}\right)} \\
			&= MN \cdot \mathcal{D}^{-1}\left\{ FH\right\}
		\end{align}
		
		where
		\begin{equation}
			\hat{\delta}_N (k) = \begin{cases}
				1 & \text{when } k = 0 \mod N \\
				0 & \text{else}
				\end{cases}
		\end{equation}
	\end{proof}
	Above, we make use of the following lemma:
	\begin{lemma}
	Let $j$ and $k$ be integers and let $N$ be a positive integer. Then
	\begin{equation}
		\sum_{n=0}^{N-1} e^{2\pi i\left(\frac{n(j-k)}{N}\right)} =  N \cdot \hat{\delta}_N(j-k)
		\end{equation}
		\end{lemma}
		\begin{proof}
		
		For any particular $n \in 0..N-1$,consider the complex number $e^{2\pi i n(j-k)/N}$. Note first, that this is an $N$-th root of unity, since
		\[
		\left(e^{2\pi i n(j-k)/N}\right)^N = e^{2\pi i n(j-k)} = \left(e^{2\pi i}\right)^{n(j-k)}
		= 1^{n(j-k)} = 1
		\]
	
	Thus, we understand that the complex number $e^{2\pi i n(j-k)/N}$ is a root of $z^N -1 = 0$, which we rewrite as
	\[ z^N -1 \;=\; (z-1)(z^{n-1} + \cdots + z + 1) \;=\; (z-1)\sum_{n=0}^{N-1} z^n .
	\]

We consider two cases: in the case that $j-k$ is a multiple of $N$, we of course have $e^{2\pi i n(j-k)/N} = 1$ for any $n$, and thus
\[
\sum_{n=0}^{N-1} e^{2\pi i\left(\frac{n(j-k)}{N}\right)} = \sum_{n=0}^{N-1} \left(1\right) = N
\].

In the case that $j-k$ is \textit{not} a multiple of $N$, then clearly $\left(e^{2\pi i n(j-k)/N}\right)-1 \ne 0$, and thus it must be that
\[\left(e^{2\pi i n(j-k)/N}\right)^N -1 = 0 \;\implies\; \sum_{n=0}^{N-1} \left(e^{2\pi i n(j-k)/N}\right)^n = 0\]
	
	Combining these two cases gives the result of the lemma.
		\end{proof}
				
		\subsubsection{FFT}
		
		As noted, the above result applies to the Discrete Fourier Transform. As noted, we actually achieve a convolution speedup using a Fast Fourier Transform (FFT) instead.
		
		\begin{itemize}
			\item Basic theory.
			\item Show speedup and equivalency.
		\end{itemize}
		
		\subsubsection{Odds \& Ends of Fourier Analysis}
		\begin{itemize}
			\item Sampling theory?
			\item Wraparound error?
			\item Any other kinks introduced in going from continuous to discrete.
		\end{itemize}


\hrulefill
\section{Linear Scale Space Theory}
	Koenderink showed/asserted that "any image can be embedded in a one-parameter family of derived images (with resolution as the parameter) in essentially only one unique way" given a few of the so-called \textit{scale space axioms}. They showed in particular that any such family must satisfy the heat equation
	\begin{equation}
		\Delta K(x,y,\sigma) = K_\sigma (x,y,\sigma) 
		\;\text{for}\; \sigma \ge 0
		\;\text{such that}\; K(x,y, 0) = u_0(x,y).
		\end{equation}
		where $K: \R^3 \to \R $ and  $u_0: \R^2 \to \R: $ is the original image (viewed as a continuous surface) and $\sigma$ is a resolution parameter.
	Much work blah blah blah has been done to formalize this approach. 
	
\subsection{Significance of the convolution operation}
	This section shows that if one constructs the family using an operation and a source image, the necessary operation is necessarily equivalent to convolution. I am guessing at this, but I think the axiom that causes this is wanting no space in the image to be preferred.
\subsection{Uniqueness of the Gaussian Kernel}
	This is the result of most of these axioms.
	

\subsection{$G_\sigma \star u_0$ solves the heat equation}
	given $u_0$ as a continuous image (unscaled), we construct PDE with this as a boundary condition.
	
	\begin{equation}
		u: \R^2 \supset \Omega \to \R \; \textrm{with} \; u(\bm{x},t) : \;
			\begin{cases}
				\frac{\partial u}{\partial t} (\bm{x}, t) = \Delta u(\bm{x},t) & ,\; t \ge 0 \\
				u(\bm{x},0) = u_0(\bm{x}) 
			\end{cases}
		\end{equation}

	We show that
	\begin{equation}
		u(\bm{x},t) = \left(G_{\sqrt{2t}} \star u_0 \right)(\bm{x})
	\end{equation}
	solves (the above tagged equation), where
	\[
		G_\sigma := \frac{1}{2\pi \sigma^2} e^{\left(-\abs{x}^2 / (2\sigma^2)\right)}
	\]
	First, we need a quick lemma regarding differentiation a continuous convolution.
	\begin{lemma} \label{dconvolution}
		Derivative of a convolution is the way that it is (obviously rewrite this).
	\end{lemma}
	\begin{proof}
		For a single variable,
		\begin{align}
		\frac{\partial}{\partial \alpha} \left[ f(\alpha) \star g(\alpha) \right]
		&= \frac{\partial}{\partial \alpha} \left[ 
		\int f(t) g(\alpha - t) dt \right] \\
		&=  \int f(t) \frac{\partial}{\partial \alpha}\left[ g(\alpha - t)  \right] dt \\
		&=  \int f(t) \left(\frac{\partial g}{\partial \alpha}\right) g(\alpha - t) dt \\
		&=  f(\alpha) \star g'(\alpha)
		\end{align}
		By symmetry of convolution we can also conclude 
		\[\frac{\partial}{\partial \alpha} \left[ f(\alpha) \star g(\alpha) \right]
		= f'(\alpha) \star g(\alpha)
		\]
		
		If $f$ and $g$ are twice differentiable, we can compound this result to show a similar statement holds for second derivatives, and then, given the additivity of convolution,
		we may conclude
		\begin{equation}
		\Delta \left(f \star g \right) = \Delta(f) \star g = f \star \Delta(g) 
		\end{equation} 
	\end{proof}
	\begin{theorem}
		$u(\bm{x},t) = \left(G_{\sqrt{2t}} \star u_0 \right)(\bm{x})$ solves the heat equation.
	\end{theorem}
	\begin{proof}
		We focus on the particular kernel
		\[
			G_{\sqrt{2t}} = \frac{1}{4\pi t} e^{\left(-\abs{x}^2 / (4t)\right)}
			\]
	
	
	Then
	\begin{align}
	\frac{\partial u}{\partial t} (\bm{x}, t)
		&= \frac{\partial}{\partial t} \left(G_{\sqrt{2t}}(\bm{x},t) \star u_0(\bm{x})\right)  \\
		&= \frac{\partial}{\partial t} \left(G_{\sqrt{2t}}(\bm{x},t)\right) \star u_0(\bm{x})  \\
		&= \frac{\partial}{\partial t} \left(
			\frac{1}{4\pi t} e^{\left(-\abs{x}^2 / (4t)\right)} \right) \star u_0(\bm{x}) \\
		&= \left[
		-\frac{1}{4\pi t^2} e^{\left(-\abs{x}^2 / (4t)\right)}
			+ \frac{1}{4\pi t}\left(\frac{-\abs{x}^2}{4t^2}\right) e^{-\abs{x}^2 / (4t)}
			\right] \star u_0(\bm{x}) \\
			&= -\frac{1}{4t^2} \left( e^{\left(-\abs{x}^2 / (4t)\right)} 
					+ \abs{\bm{x}}^2 G_{\sqrt{2t}}(\bm{x},t)
						\right) \star u_0(\bm{x})
	\end{align}
	and from the previous lemma,
	\[
	\Delta u(\bm{x}, t) = \Delta\left( G_{\sqrt{2t}} \star u_0(\bm{x})\right)
						= \Delta\left( G_{\sqrt{2t}} \right)\star u_0(\bm{x})
	\]
	
	We explicitly calculate the Laplacian of $G_{\sigma}(x,y) = A \exp(-\frac{x^2 + y^2}{2\sigma^2})$ as follows:
	
	\begin{align*}
	\frac{\partial}{\partial x} G_{\sigma}(x,y)
		&= A \left( \frac{-2x}{2\sigma^2}\right) \exp\left(-\frac{x^2 + y^2}{2\sigma^2}\right) \\
		\implies \frac{\partial^2}{\partial^2 x} G_{\sigma}(x,y)
		&= A \cdot \frac{\partial}{\partial x}
		\left[ - \frac{x}{\sigma^2} \exp\left(-\frac{x^2 + y^2}{2\sigma^2}\right) \right] \\
		&= A \left[ - \frac{1}{\sigma^2} \exp\left(-\frac{x^2 + y^2}{2\sigma^2}\right) 
			+ \frac{x}{\sigma^2} \cdot \frac{2x}{2\sigma^2} \exp\left(-\frac{x^2 + y^2}{2\sigma^2}\right) \right] \\
			&= A \exp\left(-\frac{x^2 + y^2}{2\sigma^2}\right)
				\left[ - \frac{1}{\sigma^2} + \frac{x^2}{\sigma^4} \right] \\
				&= \frac{1}{\sigma^2} G_\sigma(x,y)  \left[ \frac{x^2}{\sigma^2} - 1\right]
	\end{align*}
	
	By symmetry of argument we also may conclude
	\[
	\frac{\partial^2}{\partial y^2} G_{\sigma}(x,y) = \frac{1}{\sigma^2} G_\sigma(x,y)  \left[ \frac{y^2}{\sigma^2} - 1\right]
	\]
	
	and so
	
	\begin{equation}
	\Delta G_\sigma(x,y) =
		\frac{\partial^2}{\partial x^2} \left(G_{\sigma}\right)
		+ \frac{\partial^2}{\partial y^2} \left(G_{\sigma}\right)
		= \frac{1}{\sigma^2} G_\sigma(x,y) \left[ \frac{x^2 + y^2}{\sigma^2} - 2\right] 
	\end{equation}
	Then, given \cref{dconvolution}, we conclude
	\begin{equation}
	\Delta \left[ G_\sigma(x,y) \star u_0(x,y) \right] 
	= \left(\frac{1}{\sigma^2} G_\sigma(x,y) \left[ \frac{x^2 + y^2}{\sigma^2} - 2\right]\right) \star u_0(x,y)
	\end{equation}
	
	For particular choices of $\sigma(t) = \sqrt{2t}$ and $A = \frac{1}{4\pi t}$,
	we see 
	\begin{align}
		\Delta \left[ G_{\sqrt{2t}}(x,y) \star u_0(x,y) \right] 
		&= \left(\frac{1}{2t} G_{\sqrt{2t}}(x,y) \left[ \frac{x^2 + y^2}{2t} - 2\right]\right) \star u_0(x,y) \\
		&= \left(G_{\sqrt{2t}}(x,y) \left[ \frac{x^2 + y^2}{4t^2} - \frac{1}{t}\right]\right) \star u_0(x,y)
	\end{align}
	We then calculate the time derivative,
	using our particular choice of $\sigma(t) = \sqrt{2t}$ and $A = \frac{1}{4\pi t}$ as:
	
	\begin{align}
	\frac{\partial}{\partial t} \left[ G_{\sigma(t)}(x,y) \star u_0(x,y) \right]
	&= \frac{\partial}{\partial t} \left[ G_{\sigma(t)}(x,y) \right] \star u_0(x,y) \\
	&= \frac{\partial}{\partial t} \left[ G_{\sqrt{2t}}(x,y)\right] \star u_0(x,y) \\
	&= \frac{\partial}{\partial t} \left[
	\frac{1}{4\pi t} \exp\left(-\frac{x^2 + y^2}{4t}\right) \right] \star u_0(x,y) \\
	&= \left[ -\frac{1}{4\pi t^2} \exp\left(-\frac{x^2 + y^2}{4t}\right) + 
	\frac{1}{4\pi t}\left( \frac{x^2 + y^2}{4t^2} \exp\left(-\frac{x^2 + y^2}{4t}\right)\right)
		\right] \star u_0(x,y) \\
		&= \left(G_{\sqrt{2t}}(x,y) \left[ \frac{x^2 + y^2}{4t^2} -\frac{1}{t}\right]\right) \star u_0(x,y)
	\end{align}

	Combining these results, we find that
	\begin{equation}
	\frac{\partial}{\partial t} \left[ G_{\sqrt{2t}} \star u_0 \right]
	= \Delta \left[ G_{\sqrt{2t}} \star u_0 \right] 
	\end{equation}
	
	as desired. \end{proof}
\hrulefill
\section{Morphology}
	Methods of merging multiscale methods.
	
	
\hrulefill
\section{Other Odds and Ends}
	Finding the placental plate. Preprocessing. Could go in next section as well.



