\chapter{Mathematical Methods}

\section{Overview of Differential Geometry in Image Processing}
	\subsection{Basics, Definitions}
        \begin{defn}
    	For theoretical purposes, we may view any 2D grayscale image
    	as a continuous function $L: \R^2 \to \R$
        with $L \in \mathcal{C}^2\left(\R^2\right).$
        \end{defn}
        \begin{defn}
        In the context of differential geometry, we wish to refer to its graph
            $f: \R^2 \to \R^3 $ by $ (u,v) \mapsto (u, v, L(u,v))$.
    	\end{defn}
    % put this later instead?
        \begin{defn}
    In situations where we wish to discuss a discrete image, we may refer to
	$L_0 \in \R^{m\times n}$. That is, $L_0$ is a matrix corresponding to the $m$-by-$n$ digital grayscale image. 
    \end{defn}
	
	
	Viewing the surface in $R^3$, we define the Hessian $\Hess$ of the surface $L$
	at a point $(x,y)$ on the surface as the matrix of its second partial derivatives:
	\begin{equation}
\Hess(x,y) = \begin{bmatrix}
		L_{xx}(x,y) & L_{xy}(x,y) \\
		L_{yx}(x,y) & L_{yy}(x,y)
	\end{bmatrix}
	\end{equation}
	
	At any point $(x,y)$ we denote the two eigenpairs of $\Hess(x,y)$ as
	\begin{equation}
		\Hess u_i = \kappa_i u_i \; , \quad i = 1,2
	\end{equation}
	
	where $\kappa_i$ and $u_i$ are known as the
	\textit{principal curvatures} and \textit{principal directions} of $L(x,y)$, respectively, and we label such that $|\kappa_2| \ge |\kappa_1|$. Notably, $\Hess(x,y)$ is a real, symmetric matrix (since  $L_{xy} = L_{yx}$ and $L$ is a real function)and thus its eigenvalues are real and its eigenvectors are orthonormal to each other, as given by following lemma:
	
        % from burden and faires corollary 9.17
        \begin{lemma}[Principal Axis Theorem?]
		Let $A$ be a real, symmetric matrix. The eigenvalues of $A$ are real and its eigenvectors are orthonormal to each other.
	\end{lemma}
	
	\begin{proof}
		Let $x\ne 0$ so that $Ax = \lambda x$. Then 
		\begin{align*}
		\vnorm{Ax}_2^2 = \inner{Ax}{Ax}  &= (Ax)^{*} Ax \\
						&= x^{*}A^{*}Ax = x^{*}A^T A x = x*A A x \\
						&= x^{*} A \lambda x = \lambda x^{*} A x \\
						&= \lambda x^{*} \lambda x = \lambda^2 x^{*} x = \lambda ^2 \vnorm{x}_2^2
		\end{align*}
		Upon rearrangement, we have
		$\lambda^2 = \frac{\vnorm{Ax}_2^2}{\vnorm{x}_2^2} \ge 0 \implies \lambda $ is real.
		
		To prove that a set of orthonormalizable eigenvectors exists,
		let $A$ be real, symmetric as above and consider the eigenpairs
		$Av_1 = \lambda_1 v_1$, $Av_2 = \lambda_2 v_2$ with $v_1, v_2 \ne 0$.
		\footnote{To simplify notation, we simplify our argument to consider two explicit eigenvectors only, since we're only concerned with the $2\times 2$ matrix $\Hess$
			anyway.}
		
		In the case that $\lambda_1 \ne \lambda_2$, we have
		\begin{align*}
			(\lambda_1 - \lambda_2)v_1^T v_2 &= \lambda_1 v_1^T v_2 - \lambda_2 v_1^T v_2 \\
										  	 &= (\lambda_1 v_1)^T v_2 - v_1^T (\lambda_2 v_2) \\
										  	 &= (Av_1)^T v_2 - v_1^T (Av_2) \\
										  	 &= v_1^T A^T v_2 - v_1^T A v_2 \\
										  	 &= v_1^T A v_2 - v_1^T A v_2 = 0
			\end{align*}
			Since $\lambda_1 \ne \lambda_2$, we conclude that $v_1^T v_2 = 0$.
		
		In the case that $\lambda_1 = \lambda_2 =: \lambda$, we can define
		(as in Gram-Schmidt orthogonalization) $u = v_2 - \frac{v_1^Tv_2}{v_1^Tv_1}v_1$.
		This is an eigenvector for $\lambda=\lambda_2$, as
		\begin{align*}
		Au &= A\left(v_2 - \frac{v_1^Tv_2}{v_1^Tv_1} v_1\right) \\
		   &= A v_2 - \frac{v_1^Tv_2}{v_1^Tv_1} A v_1 \\
		   &= \lambda v_2- \frac{v_1^Tv_2}{v_1^Tv_1} \lambda v_1 \\
		   &= \lambda \left( v_2 - \frac{v_1^Tv_2}{v_1^Tv_1} v_1 \right) = \lambda u
		\end{align*}
		and is perpendicular to $v_1$, since
			\begin{align*}
			v_1^T u &= v_1^T\left(v_2 - \frac{v_1^Tv_2}{v_1^Tv_1}v_1\right) \\
					&= v_1^T v_2 - \left(\frac{v_1^Tv_2}{v_1^Tv_1}\right) v_1^T v_1 \\
					&= v_1^T v_2 - v_1^Tv_2 (1) = 0.
			\end{align*}
	\end{proof}
	
        Thus we see that the two principal directions form an orthonormal frame at each point (x,y) within the continuous image $L(x,y)$.

        \maltese The following is an \textbf{unverified claim} (which might be useful for later):
            The frame varies continuously along paths in $\R^2$
            except at points where $\Hess(x,y)$ is singular.
        To make this explicit:

        \begin{theorem}[Continuity of the leading principal direction]
            Let $\theta: I:= [0,1] \to \R^2$ be a parametrized regular curve in $\R^2$ and
            $H_\theta := \Hess_f  \circ \theta(t)$ be the matrix-valued function
            % use the same notation as the Hessian before and cite the eqn. number from earlier
            (where $\Hess_f$ is the $2\times 2$ Hessian of the smooth surface $f$)
            Let $U:I \to \R^2$ be the implicitly-defined vector valued function s.t.
            $U(t)$ is the leading eigenvector of $H_\theta$
            (and therefore the leading principal direction of $f$). That is,
            \begin{equation}
                H_\theta \; U(t) = \lambda U(t) \quad \textrm{with}\quad \lambda = \rho(H_\theta)
            \end{equation}
            In other words, $\abs{\lambda} \ge \abs{\tilde{\lambda}}$ for any
            $\tilde{\lambda} : H_\theta \; u = \tilde{\lambda} u$ for some $u \ne 0$.
            
            Then, $U(t)$ is continuous in $t$ whenever $H_f(t)$ is non-singular.
            \textit{Note:} Maybe fix this so that the path avoids any nonsingular points?
            $U(t)$ isn't even well-defined at such points anyway.
        \end{theorem}
        \begin{proof}
            First, we show that $U(t)$ is a well-defined function at all points $t$ where
            $H_f(t)$ is non-singular.
        \end{proof}

         \maltese \textbf{TODO}       

\hrulefill

\section{Eigenvectors of the Hessian are principal curvature directions}
%	\maltese \textbf{TO-DO \color{red}REWRITE THIS AS A PARAGRAPH AND/OR INTERSPERSE THROUGHOUT THE SECTION}
%	
	
%	Formally, within the context of differential geometry, we should show a few things:
%	\begin{itemize}
%		\item definition of curvature of a surface (how it relates to curvature of a regular curve) and showing that it's invariant of which curve is taken--that is, it only depends on a tangent vector to the surface.
%		\item normal curvature is given by the second fundamental form
%			$\textbf{II}(X,X) = \inner{\mathsf{L}X}{X}$ where $\mathsf{L}$ is the Weingarten map.
%		\item $X$ is an extremal value of $\textbf{II}(X,X)$ subject to $\textbf{I}(X,X) = 1 $ iff $X$ is an eigenvector of $L$.
%		\item In the case of a surface $f(u,v) = (u,v,h(u,v))$ (i.e. graph of a 2D function, which is the only situation we care about in image processing), the Weingarten map is exactly the Hessian matrix.
%		\end{itemize}
        
        \subsection{Preliminaries of Differential Geometry / Notational Dump}
        
        Given an open subset $U\subset \R^2$ and a twice differentiable function  $h: U \to \R$.
        then we define its \textit{graph} $f$ as
        \[
	        f: U \to \R^3 \quad \textrm{by} \quad f(u_1, u_2) \;=\; \big(u_1 , u_2 , h(u_1,u_2)\big)
	        \quad,\; u = (u_1, u_2) \in U
        \]
        
        For any point $u \in U$, we associate a $p \in f[U]$, i.e. $p = f(u)$.
        
        We define the tangent plane of f at point $p$ as the map
        \[
        T_u f := Df\vert_u \left(T_u U\right) \subset T_{f(u)} \R^3
        \]
        
        where $Df\vert_x$ is the differential map of $f$ at point $x$ given by
        
        \[
	        Df|_x : U \to \R^3 \quad \textrm{by} \quad v \mapsto J_f (x) \cdot v
        \]
	  where $J_f(x)$ is the Jacobian of $f$ evaluated at point $x$, i.e. the matrix
	  \[
	  J_f (x) = \left[ \left.\frac{\partial f_i}{\partial u_j}\right\vert_x \right]_{i,j}
	  \]
	  
	  
	  We shall denote a tangent vector $X \in T_u f$ at point $p$. It is easily shown that we may expand any such vector $X$ in terms of the basis $\left\{ \frac{\partial f}{\partial u_i}\right\}_{i=1,2}$; that is,
	  $\textrm{span}\left\{ \frac{\partial f}{\partial u_1}, \frac{\partial f}{\partial u_2}\right\} = T_u f$.
	  
	  
	  Given a closed interval $I \subset \R$, we can define a regular curve on the surface
	  $c: I \to \R^3$ such that $\image(c) \subset \image(f)$. In other words, we can (for this particular curve) define an intermediary parametrization $\theta_c$ for this curve so that
      $ c = f \circ \theta_c $ i.e.
	  \[
	  \theta_c : I \to U \; \textrm{by} \; \theta(t) = \big(\theta_1(t), \theta_2(t)\big)
	  \]
	  
	  and $c(t) = f(\theta(t))$.
	  
	  We also shall define at a point $p = f(u)$ the Gauss map
	  \[\nu : U \to S^2 \quad\textrm{by}\quad  \nu(u_1, u_2) :=
	  \frac{\frac{\partial f}{\partial u_1} \times \frac{\partial f}{\partial u_2}}
	  {\vnorm{\frac{\partial f}{\partial u_1} \times \frac{\partial f}{\partial u_2}}} \]
	  
	  with each partial above understood to be evaluated at the input $u \in U$.
	  
	  It is clear that $\nu \perp \frac{\partial f}{\partial u_i}$ each $i=1,2$, and also that
	  $\textrm{span}\left\{ \frac{\partial \nu}{\partial u_1}, \frac{\partial \nu}{\partial u_2}\right\} = T_u f$ as well.
        \subsection{Defining normal curvature of a surface}
        In the context of a regular arc-length parametrized curve $c: I \to \R^3$ parametrized along some closed interval $I\in \R$
	        (that is, a differentiable, one-to-one curve where $c'(s) = 1 \;\; \forall s \in I$), curvature at a point $s \in I$ is defined simply as the magnitude of the curve's acceleration: $\kappa(s) := \vnorm{c''(s)}$.
	    
	    To extend the notion of curvature of a surface $f$, we can consider the curvature of such a curve embedded within the surface: that is, $c[I] = f[\theta_c[I]]$. Considering a point $p \in I$ and its associated point $u = \theta_c(p)$, we wish to compare the curvatures of all curves at the point with a shared velocity. We now present a main result that provides a notion of curvature of a surface.
		
%		\textit{\maltese TODO \color{red} fix so this is clearer and less bogged down by 
%			$u \leftrightarrow p$ conversion so you can more easily tell what all curves have in
%			common}
		\begin{theorem}[Theorem of Meusnier]
			Given a point $u \in U $ and a tangent direction $X \in T_u f$,
		    any curve on the surface $c: I \to \image(f)$ with $p\in I : \theta_c(p) = u$
		    where $c'(p) = X$ will have the same curvature.
		\end{theorem}
		
		\begin{proof}
		Considering any such curve where $\frac{\partial c}{\partial t}(p) = X$ where $X \in T_u \R^3$ is a normalized vector tangent to the surface at the point $u$,
		we wish to decompose its acceleration along the  orthogonal vectors $X$ and
		the Gauss map $\nu := \nu(u_1, u_2) =
			\frac{\frac{\partial f}{\partial u_1} \times \frac{\partial f}{\partial u_2}}
			{\vnorm{\frac{\partial f}{\partial u_1} \times \frac{\partial f}{\partial u_2}}}$. (Note that $X$ and $\nu$ are indeed orthogonal,
			as both $\frac{\partial f}{\partial u_i} \in T_u \R^3, \; i=1,2.$) We then have
			(at this fixed point $u=\theta_c(p)$)
			
			\begin{equation}
				c'' = \inner{c''}{X} X + \inner{c''}{\nu}\nu
				\end{equation}. 
		
		The first term is zero, since for a regular curve
		\[ \inner{c''}{X} = \inner{c''}{c'} = 0 \]
		
		since either $c'' \perp c'$ or $\vnorm{c''} = 0$ itself (which is generally true for a regular curve in $\R^3$) %see Kuhnel pg. 13, def 2.4
		
		Using chain rule, we can rewrite the second coefficient of (2.4) as % prove chain rule?
		\begin{align}
			\inner{c''}{\nu} &=
			\frac{\partial}{\partial t}\left[ \inner{c'}{\nu} \right]
				- \inner{c'}{\frac{\partial \nu}{\partial t}} \\
				&= \frac{\partial}{\partial t}\left[ \inner{X}{\nu} \right]
				- \inner{c'}{\frac{\partial \nu}{\partial t}} \\
				&= 0 - \inner{X}{\frac{\partial \nu}{\partial t}}
				%&= \inner{X}{\wein X} = \mathbf{II}(X,X)
				\end{align}
		
		Thus, we can express the curvature at this point on our selected curve as
		\begin{align}
		%\vnorm{c''} = \mathbf{II}(X,X) \vnorm{\nu} = \mathbf{II}(X,X)
		\vnorm{c''} = \vnorm{\inner{c''}{X} X + \inner{c''}{\nu}\nu}
		&= \vnorm{0 + \inner{c''}{\nu}\nu} \\
		&= - \inner{X}{\frac{\partial \nu}{\partial t}}\vnorm{\nu} \\
		&= - \inner{X}{\frac{\partial \nu}{\partial t}} \\
		&=  \inner{X}{-\frac{\partial \nu}{\partial t}}
		\end{align}
		
		We may compute $-\frac{\partial \nu}{\partial t}$ via chain rule:
		\begin{align}
		-\frac{d \nu}{dt} &= -\frac{d}{dt}\big[\nu(\theta_1(t), \theta_2(t))\big] \\
		&= \theta_1'(s)\left( - \frac{\partial \nu}{\partial u_1} \right) + 
		\theta_2'(s)\left( - \frac{\partial \nu}{\partial u_2} \right)
		\end{align}
		
		Using this, we can rewrite the curvature for our curve as
		\begin{align}
		\vnorm{c''} = \vnorm{\inner{c''}{X} X + \inner{c''}{\nu}\nu}
		&= \vnorm{0 + \inner{c''}{\nu}\nu} \\
		&= - \inner{X}{\frac{\partial \nu}{\partial t}}\vnorm{\nu} \\
		&= - \inner{X}{\frac{\partial \nu}{\partial t}} \\
		&=  \inner{X}{-\frac{\partial \nu}{\partial t}}
		\end{align}
		
		
		Identifying $\left\{ \frac{\partial \nu}{\partial u_i}\right\}_{i=1,2}$ as a basis for $T_u f$, just as $\left\{ \frac{\partial f}{\partial u_i}\right\}_{i=1,2}$ is, we can identify a linear transformation $\wein: T_u f \to T_u f$ which converts between bases, i.e.
		$\wein(\frac{\partial f}{\partial u_i}) = - \frac{\partial \nu}{\partial u_i}$. This allows us to rewrite the time derivative of the Gauss map as
			\begin{align}
			-\frac{d \nu}{dt} &= 
			\theta_1'(s)\left( - \frac{\partial \nu}{\partial u_1} \right) + 
			\theta_2'(s)\left( - \frac{\partial \nu}{\partial u_2} \right) \\
			&= \theta_1'(s)\left( L\left(- \frac{\partial f}{\partial u_1}\right) \right) + 
			\theta_2'(s)\left( L\left(- \frac{\partial f}{\partial u_2}\right) \right) \\
			&= L\left(\theta_1'(s)\left( - \frac{\partial f}{\partial u_1}\right)  + 
			\theta_2'(s)\left( - \frac{\partial f}{\partial u_2} \right)\right) \\
			&= L\left( \frac{d}{dt}\left[ f\left(\theta(t)\right)\right] \right)
			= L\left( \frac{d}{dt} \left[ c(t) \right] \right) = L\left(X\right)
			\end{align} 
			With this, we can re-express the curvature of our curve as
			
			\begin{equation}
			\vnorm{c''} = \inner{X}{-\frac{\partial \nu}{\partial t}} = \inner{X}{\wein(X)}
			\end{equation}
			
			which only depends on the point $u$ and the selected direction $X$, not on the particular curve at all! 
		\end{proof}
		In fact, we refer to this quantity as the normal curvature of the surface. 
		
		\begin{defn}
			The normal curvature of a surface at point $u$ in the direction $X$ is given by $\kappa_\nu := - \inner{X}{L(X)}$. In other contexts (not necessary here), this quantity is referred to as the \textit{second fundamental form} at the point $u \in U$; that is, $ \mathbf{II}(X,X) := - \inner{X}{\wein(X)}$.
		\end{defn}
		
		
		
		The map $\wein$ we used is known as the Weingarten map
		and is implicitly defined at each $u \in U$. 
		We wish to make its existence rigorous as well as find a matrix representation for it, using the standard motivation that $\wein(\frac{\partial f}{\partial u_i}) = - \frac{\partial \nu}{\partial u_i}$.
		
	
		\begin{defn}[Weingarten map]
		The Weingarten map (or shape operator) is the map $\wein: T_u f \to T_u f$ given by
		$\wein = D\nu \circ (Df)^{-1}$.
		\end{defn}
		
		That is, we may trace any $X \in T_u f$ which has been expanded in terms of the basis 
		$\left\{\frac{\partial f}{\partial u_1} , \frac{\partial f}{\partial u_2}\right\}$
		and expand it along the basis $\left\{-\frac{\partial \nu}{\partial u_1} , -\frac{\partial \nu}{\partial u_2}\right\}$. 
		
		\textcolor{red}{TODO: THIS GOES SOMEWHERE ELSE?? You can simplify by assuming we're dealing with a graph, although it's true in general as long as $f$ is an immersion.}
		To prove that $\left\{\frac{\partial f}{\partial u_1} , \frac{\partial f}{\partial u_2}\right\}$ and $\left\{-\frac{\partial \nu}{\partial u_1} , -\frac{\partial \nu}{\partial u_2}\right\}$ are each a basis for $T_u f$,
		
		For any immersion, $\left\{\frac{\partial f}{\partial u_1} , \frac{\partial f}{\partial u_2}\right\}$ is a linearly independent set. We can directly see that $f_{u_1}$ and $f_{u_2}$ are linearly independent in the case of a graph, as $f_{u_1} = (1,0,h_{u_2})$ and $f_{u_2} = (0,1,h_{u_2})$, which are clearly linearly independent.
		
		To show that $\left\{-\frac{\partial \nu}{\partial u_1} , -\frac{\partial \nu}{\partial u_2}\right\}$ is a basis as well,
		first note that at any particular $u \in U$,
		$\inner{\nu}{\nu} = 1 \implies \frac{\partial}{\partial u_i} \inner{\nu}{\nu} = 0$,
		and so by chain rule $2\inner{\frac{\partial \nu}{\partial u_i}}{\nu} = 0
		\implies \frac{\partial \nu}{\partial u_i} \perp \nu $.
			Since $ \nu \perp \spn\left\{\frac{\partial f}{\partial u_i}\right\} $ as well (since $\nu$ its outer product), in  $\R^3$, this implies
			$\spn\left\{\frac{\partial \nu}{\partial u_i}\right\} \parallel
			\spn\left\{\frac{\partial f}{\partial u_i}\right\}$.
			\textcolor{red}{TODO: why are $\nu_{u_i}$ necessarily linearly independent?}
		
		\textcolor{red}{TODO: Finish proving soundness of the Weingarten map in the context of a graph}
		
		
		To find a matrix representation for $\wein$, (which we will denote $\weinmat \in R^{2\times2}$) we simply wish to find a linear transformation
		such that
		$\weinmat \left.\frac{\partial f}{\partial u_i}\right|_{T_u f}
			= - \left.\frac{\partial \nu}{\partial u_i}\right|_{T_u f} \quad \textrm{for} i=1,2$
			
		where $- \left.X\right|_{T_u f}$ denotes that $X \in T_u f$ is being represented in a local basis of $T_u f$. In matrix form, this translates to
		
		\begin{align}
		\Bigg[ \;\weinmat\; \Bigg]
		\left[ \mcol{\left[\frac{\partial f}{\partial u_1}\right]_{T_u f}} \;
				\mcol{\left[\frac{\partial f}{\partial u_2}\right]_{T_u f}} \right]
				&= \left[ \weinmat \mcol{\left[\frac{\partial f}{\partial u_1}\right]_{T_u f}} \;
				\weinmat \mcol{\left[\frac{\partial f}{\partial u_2}\right]_{T_u f}} \right] \\
				&= \left[ \mcol{ \left[-\frac{\partial \nu}{\partial u_1}\right]_{T_u f}} \;
				\mcol{\left[-\frac{\partial \nu}{\partial u_2}\right]_{T_u f}} \right]
				\end{align}
				Now, representing each vector in  $T_u f$ with respect to the basis $\left\{ \frac{\partial f}{\partial u_i}\right\}$, we have
				\begin{align}
				\implies
				\Bigg[ \;\weinmat\; \Bigg]
				\begin{bmatrix} \leftarrow \frac{\partial f}{\partial u_1} \rightarrow \\
								\leftarrow \frac{\partial f}{\partial u_2} \rightarrow
								\end{bmatrix}
				\left[ \mcol{\frac{\partial f}{\partial u_1}} \;
					\mcol{\frac{\partial f}{\partial u_2}}\right]
					&= \begin{bmatrix} \leftarrow \frac{\partial f}{\partial u_1} \rightarrow \\
					\leftarrow \frac{\partial f}{\partial u_2} \rightarrow
					\end{bmatrix}
					\left[ \mcol{-\frac{\partial \nu}{\partial u_1}} \;
					\mcol{-\frac{\partial \nu}{\partial u_2}}\right]
		\end{align}
				 
		We can simplify this greatly by defining 
		\begin{equation}\label{fundamentalformcoefficients}
		g_{ij} := \inner{\frac{\partial f}{\partial u_i}}{\frac{\partial f}{\partial u_j}}
		\quad \textrm{and}\quad
		h_{ij} := \inner{\frac{\partial f}{\partial u_i}}{-\frac{\partial \nu}{\partial u_j}}
		\end{equation}
		
		so that
		\begin{equation}
		\Bigg[ \;\weinmat\; \Bigg]
		\begin{bmatrix} g_{11} & g_{12} \\ g_{21} & g_{22} \end{bmatrix}
		= \begin{bmatrix} h_{11} & h_{12} \\ h_{21} & h_{22} \end{bmatrix}
		\end{equation}
		
		Then we rearrange to solve for $\weinmat$ as
			\begin{equation} \label{weinmatconstruction}
			\weinmat
			\;=\; \begin{bmatrix} h_{11} & h_{12} \\ h_{21} & h_{22} \end{bmatrix}
			\left.\begin{bmatrix} g_{11} & g_{12} \\ g_{21} & g_{22} \end{bmatrix}\right.^{-1}
			\end{equation}
		
		Our final goal to is to characterize such normal curvatures.
		Namely, we wish to establish a method of determining in which directions an extremal
		normal curvature occurs.
		To do so, we shall consider the relationship between the direction $X$ and the normal curvature $\kappa_\nu$ in that direction at some specified $u$.

	
		
		First, we need the following lemma:
        \begin{lemma}
            If $A\in R^{n\times n}$ is a symmetric real matrix, $v \in R^n$
            and given the dot product $\inner{\cdot}{\cdot}$,
            we have $\nabla_{v} \inner{v}{Av} = 2Av$.
            In particular, when $A = I$ the identity matrix, we have
            $ \nabla_{v} \inner{v}{v} = 2v$.
        \end{lemma}
        
        \begin{proof}
            The result is uninterestingly obtained by tracking
            each (the `$i$th') component of
            $\nabla_{v} \inner{v}{Av}$:
            
            \begin{align}
            {\Big(\nabla_{v} \inner{v}{Av}\Big)}_{i} =
                    \frac{\partial}{\partial v_i} \Big[
                    \inner{v}{Av}\Big]
                    &=  \frac{\partial}{\partial v_i} \left[
                        \sum_{j=1}^{n} v_j {(Av)}_j\right] \\
                    &=	\frac{\partial}{\partial v_i} \left[
                    \sum_{j=1}^{n} v_j \sum_{k=1}^n a_{jk} v_k \right] \\
                    &= \frac{\partial}{\partial v_i} \left[
                    a_{ii} v_i^2 + v_i \sum_{k\ne i} a_{ik} v_k
				                 + v_i \sum_{j\ne i} a_{ji} v_j
				                 + \sum_{j\ne i} \sum_{k \ne i} v_j a_{jk} v_k \right] \\
				    &=  2 a_{ii} v_i + \sum_{k\ne i} a_{ik} v_k
				    + \sum_{j\ne i} a_{ji} v_j + 0 \\
				    &= 2 a_{ii} v_i + 2 \sum_{k\ne i} a_{ik} v_k
				     = 2 \sum_{k=1}^n a_{ik}v_{k} = 2 {\big(Av\big)}_i \\
				     &\quad \implies \nabla_{v} \inner{v}{Av} = 2Av.
            \end{align}
        \end{proof}
        
        We are now ready for the major result of this section.
        
        \begin{theorem}[Theorem of Olinde Rodrigues]
        	Fixing a point $u \in U$, a direction $X \in T_u f $ minimizes the normal curvature $\kappa_\nu = \inner{\wein X}{X}$ subject to $\inner{X}{X}= 1$
        	iff $X$ is a (normalized) eigenvector of the Weingarten map $\wein$.
        	\end{theorem}
        \begin{proof}
        	%	Recall first the definition of the first two fundamental forms,
        		%$\textbf{II}(X,X) = \inner{\wein{X}}{X}$
        	%	and $\textbf{I}(X,X) = \inner{X}{X}$.
        		
        		In the following, we will assume that $X \in T_u f$ is expanded,
        		in local coordinates, i.e. along  a two dimensional basis
        		(such as $\left\{ \frac{\partial f}{\partial u_i}\right\}_{i=1,2}$
        		) and thus can refer to $\wein$ freely as the $2\times2$ matrix $\weinmat$.
        		Using the method of Lagrange multipliers, we define the Lagrangian:
        		\begin{equation}
        		\mathscr{L}(X; \lambda) =
	        	\inner{\weinmat X}{X} - \lambda\Big(\inner{X}{X} - 1\Big) 
	        \end{equation}
	        	
	        	Extremal values occur when
	        	$\nabla_{X,\lambda} \mathscr{L}(X;\lambda) = 0$,
	        	which becomes the two equations
	        	
	        \begin{equation}
	        \left\{ \begin{aligned}
		        \nabla_X \inner{\weinmat X}{X} - \lambda \nabla_X \left( \inner{X}{X} - 1 \right) = 0 \\
		        \inner{X}{X} - 1 = 0
	        \end{aligned} \right.
	        \end{equation}
	        
	        The second requirement is simply the constraint that $X$ is normalized.
	        Using the previous lemma, we can simplify the first result as follows:
	        
	        \begin{gather}
	        \nabla_X \inner{\weinmat X}{X} - \lambda \nabla_X \left( \inner{X}{X} - 1 \right) = 0 \\
	        2\weinmat X - \lambda \left(2 X \right) = 0 \\
	        \implies \weinmat X - \lambda X = 0 \implies \weinmat X = \lambda X
	        \end{gather}
	        which implies that $X$ is an eigenvector of  $\weinmat$ with corresponding eigenvalue $\lambda$.
	        Thus the two hypotheses are exactly equivalent when $X$ is normalized. It is also worth remarking that the corresponding eigenvalue $\lambda$ is the Lagrangian multiplier itself.
        	\end{proof}
        	
        	Thus, to find the directions of greatest and least curvature of a surface at a point $u \in U$, we simply must calculate the Weingarten map and its eigenvectors. We refer to these directions as follows.
        	
        	\begin{defn}[Principal Curvatures and Principal Directions]
        		The extremal values of normal curvature of a surface at a point  $u\in U$
        		are referred to as \textbf{principal curvatures}. The corresponding directions at which normal curvature attains an extremal value are referred to as \textbf{principal directions}.
        	\end{defn}
        	
        	Our final goal is to explicitly determine a (hopefully simplified) version of the Weingarten map in the case of a graph $f(u_1,u_2) = (u_1,u_2, h(u_1,u_2))$ and calculate
        	the principal directions and curvatures in a simple example.
        	        		
        	\begin{theorem}
	        	When $f: U \to \R^3$ is given by $(x,y) \mapsto (x, y, h(x,y))$, the matrix
	        	representation of the Weingarten map \sout{is exactly the Hessian matrix given in (2.1)} is given by
	        	\begin{equation} \label{weinmatexactgraph}
	        	\weinmat = \Hess(h) \tilde{G} \;,\quad \mathrm{where} \quad
		        	\tilde{G} := \frac{1}{\sqrt{1+h_x^2 + h_y^2}}
		        	\begin{bmatrix}
			        	1 + h_y^2 & -h_x h_y \\
			        	-h_x h_y & 1 + h_x^2 \\
		        	\end{bmatrix} 
	        	\end{equation}
	        	
	        	In particular, given a point $u = (x,y) \in U \subset \R^2$ where $h_x \approx h_y \approx 0$, we
	        	have $\tilde{G} \approx \mathrm{Id}$, and thus $\weinmat \approx \Hess$.
        	\end{theorem}
        	\begin{proof}
        		First, we can (using chain rule) rewrite each component as in  \cref{fundamentalformcoefficients}:
        		 \[ h_{ij} = \inner{\frac{\partial f}{\partial u_i}}{-\frac{\partial \nu}{\partial u_j}}
        		= \inner{\frac{\partial^2 f}{\partial u_i \partial u_j}}{\nu} \]
        		
        		Now, given our particular surface $f$, we can calculate each of these components directly. We have:
        		\begin{equation}
        		\begin{gathered}
        		f_{x} = (1, 0, h_x) , \quad
        		f_{y} = (0, 1, h_y)  \\
        		f_{xx} = (0, 0, h_{xx}) , \quad
        		f_{xy} = (0, 0, h_{xy}) = f_{yx} , \quad
        		f_{yy} = (0, 0, h_{yy})
        		\end{gathered}
        		\end{equation}
        		
        		and we have the unit normal vector (Gauss map)
        		\begin{align}
        		\nu(u_1, u_2) &=
        		\frac{\frac{\partial f}{\partial x} \times \frac{\partial f}{\partial y}}
        		{\vnorm{\frac{\partial f}{\partial x} \times \frac{\partial f}{\partial y}}} \\
        		&= \frac{(1, 0, h_x) \times (0, 1, h_y)}{\vnorm{\cdots}} \\
        		&= \frac{\left( -h_x , -h_y , 1 \right)}{\sqrt{h_x^2 + h_y^2 + 1}}
			 \end{align}
			 We then calculate each $h_{ij}$ as
			 \begin{equation}
			 \begin{gathered}
			 h_{11} = \inner{\frac{\partial^2 f}{\partial x^2}}{\nu} = 
				 \frac{h_{xx}}{\sqrt{1+h_x^2 + h_y^2}} \\
				  h_{12} = \inner{\frac{\partial^2 f}{\partial x \partial y}}{\nu} = 
				  \frac{h_{xy}}{\sqrt{1+h_x^2 + h_y^2}} = h_{21} \\
				  h_{22} = \inner{\frac{\partial^2 f}{\partial y^2}}{\nu} = 
				  \frac{h_{yy}}{\sqrt{1+h_x^2 + h_y^2}} \\
			 \end{gathered}
			 \end{equation}
			 
			 and thus the first matrix in \cref{weinmatconstruction} is given by
			 
			 \begin{equation} \label{hij_exactgraph}
			 [h_{ij}] = \frac{1}{\sqrt{1+h_x^2 + h_y^2}} \,  \Hess (h)
			 \end{equation}
			 
			 To calcuate the second, we use
			 
			 
			 
			 \begin{equation}
			 \begin{gathered}
			 g_{ij} = \inner{\frac{\partial f}{\partial u_i}}{\frac{\partial f}{\partial u_j}} \\
			 g_{11} = \inner{f_x}{f_x} = 1 + h_x^2 \\
			 g_{12} = \inner{f_x}{f_y} = h_x h_y = g_{21} \\
			 g_{22} = \inner{f_y}{f_y} = 1 + h_y^2
			 \end{gathered}
			 \end{equation}
			 
			 and thus
			\begin{equation} \label{gij_exactgraph}		 
			[g_{ij}]^{-1} = \begin{bmatrix} 1 + h_x^2 & h_x h_y \\
						h_x h_y & 1 + h_y^2 \end{bmatrix}^{-1}
						\;=\;	\begin{bmatrix} 1 + h_y^2 & -h_x h_y \\
						-	h_x h_y & 1 + h_x^2 \end{bmatrix}
			\end{equation}
        	
        	Combining $[h_{ij}]$ and $[g_{ij}]^{-1}$ from \cref{gij_exactgraph} and \cref{hij_exactgraph}
        	we arrive at \cref{weinmatexactgraph}.
        	\end{proof}
        	
        	
\section{Calculating Derivatives of Discrete Images} 
	Discrete derivatives. \maltese\textbf{TODO:} Develop the following:
	\begin{itemize}
		\item Take derivative with gradient / divided difference.
		\item Derivatives should be taken on Gaussian blur (equivalent to scale space development) [Lindeberg]. That is, you can take the derivative of either the convolved image, or you can take derivatives of the Gaussian itself, \textit{then} convolve.
		\item Pseudocode for \texttt{np.gradient} which is used in calculating Hessian (code below)
					\begin{verbatim}
					gaussian_filtered = fftgauss(image, sigma=sigma)
					Lx, Ly = np.gradient(gaussian_filtered)
					Lxx, Lxy = np.gradient(Lx)
					Lxy, Lyy = np.gradient(Ly)
					\end{verbatim}
				
		\end{itemize}

\hrulefill
\section{The Frangi Filter}


    \subsection{Intro to Hessian-based filters}
	%Basic properties and observations. Why these filters make sense. Basic strengths and weaknesses.

        Hessian-based filters are a family of curvilinear filters that employ the Hessian and its eigenspaces
        to determine regions of significant curvature within an image.
        % as mentioned by Frangi, see Sato [13] and Lorenz [10] in their paper
        Several such filters exist --see Sato [13] and Lorenz[10]. These filters use information about the principal curvatures (eigenvalues of the Hessian) at each point to 
    \subsection{Overview of Frangi vesselness measure}
        The Frangi filter is a widely used [citation needed] Hessian-based filter that relies on the principal curvatures--that is,
        the eigenvalues of $\Hess_\sigma(x,y)$ at some particular scale $\sigma$ at each point (x,y) in the image. 
	\subsection{Implementation Details: Convolution Speedup via FFT}
	As described above, the actual computation of derivatives is achieved via convolution with a gaussian. In practice, this is very slow for large scales. In 
		\subsubsection{TODO}
		\begin{itemize}
			\item find image processing papers that find hessian from FFT / who uses this?
			\item with above: downsides?
			\item SIDE BY SIDE comparison?
		\end{itemize}
		
		\subsubsection{2D Discrete Fourier Transform Convolution Theorem}\footnote{the following was adapted in a large part from DFT: an owner's manual. cite?}
		
		\begin{theorem}[2D DFT Convolution Theorem]
		Given two discrete functions are sequences with the same length\footnote{If they're
			not actually the same length, DIP-GW suggests to make the final length at least
			$P = A+C-1$ and $Q = B+D-1$ in the case that the sizes are $A\times B$ and $C\times D$ for $f(x,y)$ and  $h(x,y)$ respectively. Not sure if that matters.}, that is:
		$f(x,y)$ and $h(x,y)$ for integers $0 < x < M$ and $0 < y < N$, we can take the discrete fourier transform (DFT) of each:
		\begin{align}
		F(u,v) := \mathcal{D}\{f(x,y)\} &=
						\sum_{x=0}^{M-1} \sum_{y=0}^{N-1} f(x,y)
						e^{-2\pi i \left(\frac{ux}{M} + \frac{vy}{N}\right)} \\	
		   H(u,v) := \mathcal{D}\{h(x,y)\} &=
						\sum_{x=0}^{M-1} \sum_{y=0}^{N-1} h(x,y)
						e^{-2\pi i \left(\frac{ux}{M} + \frac{vy}{N}\right)}
		\end{align}
		
		and given the convolution of the two functions
		\begin{equation}
		\left(f \star h\right)(x,y) = \sum_{m=0}^{M-1} \sum_{n=0}^{N-1} f(m,n)h(x-m,y-n)
		\end{equation}
		
		then $\left(f \star h\right)(x,y)$ and $MN\cdot F(u,v)H(u,v)$ are transform pairs, i.e.
		\begin{equation}
		\left(f \star h\right)(x,y) = \mathcal{D}^{-1}\left\{MN\cdot F(u,v)H(u,v)\right\}
		\end{equation}
		\end{theorem}
		The proof follows from the definition of convolution, substituting in the inverse-DFT of $f$ and $h$, and then rearrangement of finite sums.
		\begin{proof}
		\begin{align}
		\left(f \star h\right)(x,y) &= \sum_{m=0}^{M-1} \sum_{n=0}^{N-1} f(m,n)h(x-m,y-n) \\
		&= \sum_{m=0}^{M-1} \sum_{n=0}^{N-1}
		\left(\sum_{p=0}^{M-1} \sum_{q=0}^{N-1} F(p,q)
			e^{2\pi i \left(\frac{mp}{M} + \frac{nq}{N}\right)}\right)
			\left(\sum_{u=0}^{M-1} \sum_{v=0}^{N-1} H(u,v)
			e^{2\pi i \left(\frac{u(x-m)}{M} + \frac{v(y-n)}{N}\right)} \right) \\
		&= \left(\sum_{u=0}^{M-1} \sum_{v=0}^{N-1} H(u,v)
			e^{2\pi i \left(\frac{ux}{M} + \frac{vy}{N}\right)}\right)
			\left(\sum_{p=0}^{M-1} \sum_{q=0}^{N-1} F(p,q)
			\left(\sum_{m=0}^{M-1} e^{2\pi i \left(\frac{m(p-u)}{M}\right)}\right)
			\left(\sum_{n=0}^{N-1} e^{2\pi i \left(\frac{n(q-v)}{N}\right)}\right)\right) \\
			&= \left(\sum_{u=0}^{M-1} \sum_{v=0}^{N-1} H(u,v)
			e^{2\pi i \left(\frac{ux}{M} + \frac{vy}{N}\right)}\right)
			\left(\sum_{p=0}^{M-1} \sum_{q=0}^{N-1} F(p,q)
			\left( M \cdot \hat{\delta}_M(p-u) \right)
			\left( N \cdot \hat{\delta}_M(q-v)\right)\right) \\
			&= \left(\sum_{u=0}^{M-1} \sum_{v=0}^{N-1} H(u,v)
			e^{2\pi i \left(\frac{ux}{M} + \frac{vy}{N}\right)}\right)
			\cdot M N F(u,v) \\
			&=MN \cdot \sum_{u=0}^{M-1} \sum_{v=0}^{N-1} F(u,v) H(u,v)
			e^{2\pi i \left(\frac{ux}{M} + \frac{vy}{N}\right)} \\
			&= MN \cdot \mathcal{D}^{-1}\left\{ FH\right\}
		\end{align}
		
		where
		\begin{equation}
			\hat{\delta}_N (k) = \begin{cases}
				1 & \text{when } k = 0 \mod N \\
				0 & \text{else}
				\end{cases}
		\end{equation}
	\end{proof}
	Above, we make use of the following lemma:
	\begin{lemma}
	Let $j$ and $k$ be integers and let $N$ be a positive integer. Then
	\begin{equation}
		\sum_{n=0}^{N-1} e^{2\pi i\left(\frac{n(j-k)}{N}\right)} =  N \cdot \hat{\delta}_N(j-k)
		\end{equation}
		\end{lemma}
		\begin{proof}
		
		For any particular $n \in 0..N-1$,consider the complex number $e^{2\pi i n(j-k)/N}$. Note first, that this is an $N$-th root of unity, since
		\[
		\left(e^{2\pi i n(j-k)/N}\right)^N = e^{2\pi i n(j-k)} = \left(e^{2\pi i}\right)^{n(j-k)}
		= 1^{n(j-k)} = 1
		\]
	
	Thus, we understand that the complex number $e^{2\pi i n(j-k)/N}$ is a root of $z^N -1 = 0$, which we rewrite as
	\[ z^N -1 \;=\; (z-1)(z^{n-1} + \cdots + z + 1) \;=\; (z-1)\sum_{n=0}^{N-1} z^n .
	\]

We consider two cases: in the case that $j-k$ is a multiple of $N$, we of course have $e^{2\pi i n(j-k)/N} = 1$ for any $n$, and thus
\[
\sum_{n=0}^{N-1} e^{2\pi i\left(\frac{n(j-k)}{N}\right)} = \sum_{n=0}^{N-1} \left(1\right) = N
\].

In the case that $j-k$ is \textit{not} a multiple of $N$, then clearly $\left(e^{2\pi i n(j-k)/N}\right)-1 \ne 0$, and thus it must be that
\[\left(e^{2\pi i n(j-k)/N}\right)^N -1 = 0 \;\implies\; \sum_{n=0}^{N-1} \left(e^{2\pi i n(j-k)/N}\right)^n = 0\]
	
	Combining these two cases gives the result of the lemma.
		\end{proof}
				
		\subsubsection{FFT}
		
		As noted, the above result applies to the Discrete Fourier Transform. As noted, we actually achieve a convolution speedup using a Fast Fourier Transform (FFT) instead.
		
		\begin{itemize}
			\item Basic theory.
			\item Show speedup and equivalency.
		\end{itemize}
		
		\subsubsection{Odds \& Ends of Fourier Analysis}
		\begin{itemize}
			\item Sampling theory?
			\item Wraparound error?
			\item Any other kinks introduced in going from continuous to discrete.
		\end{itemize}


\hrulefill
\section{Linear Scale Space Theory}
	Koenderink showed/asserted that "any image can be embedded in a one-parameter family of derived images (with resolution as the parameter) in essentially only one unique way" given a few of the so-called \textit{scale space axioms}. They showed in particular that any such family must satisfy the heat equation
	\begin{equation}
		\Delta K(x,y,\sigma) = K_\sigma (x,y,\sigma) 
		\;\text{for}\; \sigma \ge 0
		\;\text{such that}\; K(x,y, 0) = u_0(x,y).
		\end{equation}
		where $K: \R^3 \to \R $ and  $u_0: \R^2 \to \R: $ is the original image (viewed as a continuous surface) and $\sigma$ is a resolution parameter.
	Much work blah blah blah has been done to formalize this approach. 
	
\subsection{Significance of the convolution operation}
	This section shows that if one constructs the family using an operation and a source image, the necessary operation is necessarily equivalent to convolution. I am guessing at this, but I think the axiom that causes this is wanting no space in the image to be preferred.
\subsection{Uniqueness of the Gaussian Kernel}
	This is the result of most of these axioms.
	

\subsection{$G_\sigma \star u_0$ solves the heat equation}
	given $u_0$ as a continuous image (unscaled), we construct PDE with this as a boundary condition.
	
	\begin{equation}
		u: \R^2 \supset \Omega \to \R \; \textrm{with} \; u(\bm{x},t) : \;
			\begin{cases}
				\frac{\partial u}{\partial t} (\bm{x}, t) = \Delta u(\bm{x},t) & ,\; t \ge 0 \\
				u(\bm{x},0) = u_0(\bm{x}) 
			\end{cases}
		\end{equation}

	We show that
	\begin{equation}
		u(\bm{x},t) = \left(G_{\sqrt{2t}} \star u_0 \right)(\bm{x})
	\end{equation}
	solves (the above tagged equation), where
	\[
		G_\sigma := \frac{1}{2\pi \sigma^2} e^{\left(-\abs{x}^2 / (2\sigma^2)\right)}
	\]
	First, we need a quick lemma regarding differentiation a continuous convolution.
	\begin{lemma} \label{dconvolution}
		Derivative of a convolution is the way that it is (obviously rewrite this).
	\end{lemma}
	\begin{proof}
		For a single variable,
		\begin{align}
		\frac{\partial}{\partial \alpha} \left[ f(\alpha) \star g(\alpha) \right]
		&= \frac{\partial}{\partial \alpha} \left[ 
		\int f(t) g(\alpha - t) dt \right] \\
		&=  \int f(t) \frac{\partial}{\partial \alpha}\left[ g(\alpha - t)  \right] dt \\
		&=  \int f(t) \left(\frac{\partial g}{\partial \alpha}\right) g(\alpha - t) dt \\
		&=  f(\alpha) \star g'(\alpha)
		\end{align}
		By symmetry of convolution we can also conclude 
		\[\frac{\partial}{\partial \alpha} \left[ f(\alpha) \star g(\alpha) \right]
		= f'(\alpha) \star g(\alpha)
		\]
		
		If $f$ and $g$ are twice differentiable, we can compound this result to show a similar statement holds for second derivatives, and then, given the additivity of convolution,
		we may conclude
		\begin{equation}
		\Delta \left(f \star g \right) = \Delta(f) \star g = f \star \Delta(g) 
		\end{equation} 
	\end{proof}
	\begin{theorem}
		$u(\bm{x},t) = \left(G_{\sqrt{2t}} \star u_0 \right)(\bm{x})$ solves the heat equation.
	\end{theorem}
	\begin{proof}
		We focus on the particular kernel
		\[
			G_{\sqrt{2t}} = \frac{1}{4\pi t} e^{\left(-\abs{x}^2 / (4t)\right)}
			\]
	
	
	Then
	\begin{align}
	\frac{\partial u}{\partial t} (\bm{x}, t)
		&= \frac{\partial}{\partial t} \left(G_{\sqrt{2t}}(\bm{x},t) \star u_0(\bm{x})\right)  \\
		&= \frac{\partial}{\partial t} \left(G_{\sqrt{2t}}(\bm{x},t)\right) \star u_0(\bm{x})  \\
		&= \frac{\partial}{\partial t} \left(
			\frac{1}{4\pi t} e^{\left(-\abs{x}^2 / (4t)\right)} \right) \star u_0(\bm{x}) \\
		&= \left[
		-\frac{1}{4\pi t^2} e^{\left(-\abs{x}^2 / (4t)\right)}
			+ \frac{1}{4\pi t}\left(\frac{-\abs{x}^2}{4t^2}\right) e^{-\abs{x}^2 / (4t)}
			\right] \star u_0(\bm{x}) \\
			&= -\frac{1}{4t^2} \left( e^{\left(-\abs{x}^2 / (4t)\right)} 
					+ \abs{\bm{x}}^2 G_{\sqrt{2t}}(\bm{x},t)
						\right) \star u_0(\bm{x})
	\end{align}
	and from the previous lemma,
	\[
	\Delta u(\bm{x}, t) = \Delta\left( G_{\sqrt{2t}} \star u_0(\bm{x})\right)
						= \Delta\left( G_{\sqrt{2t}} \right)\star u_0(\bm{x})
	\]
	
	We explicitly calculate the Laplacian of $G_{\sigma}(x,y) = A \exp(-\frac{x^2 + y^2}{2\sigma^2})$ as follows:
	
	\begin{align*}
	\frac{\partial}{\partial x} G_{\sigma}(x,y)
		&= A \left( \frac{-2x}{2\sigma^2}\right) \exp\left(-\frac{x^2 + y^2}{2\sigma^2}\right) \\
		\implies \frac{\partial^2}{\partial^2 x} G_{\sigma}(x,y)
		&= A \cdot \frac{\partial}{\partial x}
		\left[ - \frac{x}{\sigma^2} \exp\left(-\frac{x^2 + y^2}{2\sigma^2}\right) \right] \\
		&= A \left[ - \frac{1}{\sigma^2} \exp\left(-\frac{x^2 + y^2}{2\sigma^2}\right) 
			+ \frac{x}{\sigma^2} \cdot \frac{2x}{2\sigma^2} \exp\left(-\frac{x^2 + y^2}{2\sigma^2}\right) \right] \\
			&= A \exp\left(-\frac{x^2 + y^2}{2\sigma^2}\right)
				\left[ - \frac{1}{\sigma^2} + \frac{x^2}{\sigma^4} \right] \\
				&= \frac{1}{\sigma^2} G_\sigma(x,y)  \left[ \frac{x^2}{\sigma^2} - 1\right]
	\end{align*}
	
	By symmetry of argument we also may conclude
	\[
	\frac{\partial^2}{\partial y^2} G_{\sigma}(x,y) = \frac{1}{\sigma^2} G_\sigma(x,y)  \left[ \frac{y^2}{\sigma^2} - 1\right]
	\]
	
	and so
	
	\begin{equation}
	\Delta G_\sigma(x,y) =
		\frac{\partial^2}{\partial x^2} \left(G_{\sigma}\right)
		+ \frac{\partial^2}{\partial y^2} \left(G_{\sigma}\right)
		= \frac{1}{\sigma^2} G_\sigma(x,y) \left[ \frac{x^2 + y^2}{\sigma^2} - 2\right] 
	\end{equation}
	Then, given \cref{dconvolution}, we conclude
	\begin{equation}
	\Delta \left[ G_\sigma(x,y) \star u_0(x,y) \right] 
	= \left(\frac{1}{\sigma^2} G_\sigma(x,y) \left[ \frac{x^2 + y^2}{\sigma^2} - 2\right]\right) \star u_0(x,y)
	\end{equation}
	
	For particular choices of $\sigma(t) = \sqrt{2t}$ and $A = \frac{1}{4\pi t}$,
	we see 
	\begin{align}
		\Delta \left[ G_{\sqrt{2t}}(x,y) \star u_0(x,y) \right] 
		&= \left(\frac{1}{2t} G_{\sqrt{2t}}(x,y) \left[ \frac{x^2 + y^2}{2t} - 2\right]\right) \star u_0(x,y) \\
		&= \left(G_{\sqrt{2t}}(x,y) \left[ \frac{x^2 + y^2}{4t^2} - \frac{1}{t}\right]\right) \star u_0(x,y)
	\end{align}
	We then calculate the time derivative,
	using our particular choice of $\sigma(t) = \sqrt{2t}$ and $A = \frac{1}{4\pi t}$ as:
	
	\begin{align}
	\frac{\partial}{\partial t} \left[ G_{\sigma(t)}(x,y) \star u_0(x,y) \right]
	&= \frac{\partial}{\partial t} \left[ G_{\sigma(t)}(x,y) \right] \star u_0(x,y) \\
	&= \frac{\partial}{\partial t} \left[ G_{\sqrt{2t}}(x,y)\right] \star u_0(x,y) \\
	&= \frac{\partial}{\partial t} \left[
	\frac{1}{4\pi t} \exp\left(-\frac{x^2 + y^2}{4t}\right) \right] \star u_0(x,y) \\
	&= \left[ -\frac{1}{4\pi t^2} \exp\left(-\frac{x^2 + y^2}{4t}\right) + 
	\frac{1}{4\pi t}\left( \frac{x^2 + y^2}{4t^2} \exp\left(-\frac{x^2 + y^2}{4t}\right)\right)
		\right] \star u_0(x,y) \\
		&= \left(G_{\sqrt{2t}}(x,y) \left[ \frac{x^2 + y^2}{4t^2} -\frac{1}{t}\right]\right) \star u_0(x,y)
	\end{align}

	Combining these results, we find that
	\begin{equation}
	\frac{\partial}{\partial t} \left[ G_{\sqrt{2t}} \star u_0 \right]
	= \Delta \left[ G_{\sqrt{2t}} \star u_0 \right] 
	\end{equation}
	
	as desired. \end{proof}
\hrulefill
\section{Morphology}
	Methods of merging multiscale methods.
	
	
\hrulefill
\section{Other Odds and Ends}
	Finding the placental plate. Preprocessing. Could go in next section as well.



