\chapter{Mathematical Methods}

\textit{(Note to self: this can be 50 pages long. Don't focus on brevity--rather make it basically self-contained, within reason)}


\section{Overview of Differential Geometry in Image Processing}
	\subsection{Basics, Definitions}
	For theoretical purposes, we may view any 2D grayscale image
	as a continuous surface $L: \R^2 \to \R$
	with $L \in \mathcal{C}^2\left(\R^2\right).$
	In situations where we wish to discuss a discrete image, we may refer to
	$L_0 \in \R^{m\times n}$. That is, $L_0$ is a matrix corresponding to the $m$-by-$n$ digital grayscale image.
	
	
	Viewing the surface in $R^3$, we define the Hessian $\Hess$ of the surface $L$
	at a point $(x,y)$ on the surface as the matrix of its second partial derivatives:
	\begin{equation}
\mathcal{H}(x,y) = \begin{bmatrix}
		L_{xx}(x,y) & L_{xy}(x,y) \\
		L_{yx}(x,y) & L_{yy}(x,y)
	\end{bmatrix}
	\end{equation}
	
	At any point $(x,y)$ we denote the two eigenpairs of $\Hess(x,y)$ as
	\begin{equation}
		\Hess u_i = \kappa_i u_i \; , \quad i = 1,2
	\end{equation}
	
	where $\kappa_i$ and $u_i$ are known as the
	\textit{principal curvatures} and \textit{principal directions} of $L(x,y)$, respectively, and we label such that $|\kappa_2| \ge |\kappa_1|$. Notably, $\Hess(x,y)$ is a real, symmetric matrix (since  $L_{xy} = L_{yx}$ and $L$ is a real function)and thus its eigenvalues are real and its eigenvectors are orthonormal to each other, as given by following lemma:
	
	% from burden and faires corllary 9.17
	\begin{lemma}
		Let $A$ be a real, symmetric matrix. The eigenvalues of $A$ are real and its eigenvectors are orthonormal to each other.
	\end{lemma}
	
	\begin{proof}
		Let $x\ne 0$ so that $Ax = \lambda x$. Then 
		\begin{align*}
		\vnorm{Ax}_2^2 = \inner{Ax}{Ax}  &= (Ax)^{*} Ax \\
						&= x^{*}A^{*}Ax = x^{*}A^T A x = x*A A x \\
						&= x^{*} A \lambda x = \lambda x^{*} A x \\
						&= \lambda x^{*} \lambda x = \lambda^2 x^{*} x = \lambda ^2 \vnorm{x}_2^2
		\end{align*}
		Upon rearrangement, we have
		$\lambda^2 = \frac{\vnorm{Ax}_2^2}{\vnorm{x}_2^2} \ge 0 \implies \lambda $ is real.
		
		To prove that a set of orthonormalizable eigenvectors exists,
		let $A$ be real, symmetric as above and consider the eigenpairs
		$Av_1 = \lambda_1 v_1$, $Av_2 = \lambda_2 v_2$ with $v_1, v_2 \ne 0$.
		\footnote{To simplify notation, we simplify our argument to consider two explicit eigenvectors only, since we're only concerned with the $2\times 2$ matrix $\Hess$
			anyway.}
		
		In the case that $\lambda_1 \ne \lambda_2$, we have
		\begin{align*}
			(\lambda_1 - \lambda_2)v_1^T v_2 &= \lambda_1 v_1^T v_2 - \lambda_2 v_1^T v_2 \\
										  	 &= (\lambda_1 v_1)^T v_2 - v_1^T (\lambda_2 v_2) \\
										  	 &= (Av_1)^T v_2 - v_1^T (Av_2) \\
										  	 &= v_1^T A^T v_2 - v_1^T A v_2 \\
										  	 &= v_1^T A v_2 - v_1^T A v_2 = 0
			\end{align*}
			Since $\lambda_1 \ne \lambda_2$, we conclude that $v_1^T v_2 = 0$.
		
		In the case that $\lambda_1 = \lambda_2 =: \lambda$, we can define
		(as in Gram-Schmidt orthogonalization) $u = v_2 - \frac{v_1^Tv_2}{v_1^Tv_1}v_1$.
		This is an eigenvector for $\lambda=\lambda_2$, as
		\begin{align*}
		Au &= A\left(v_2 - \frac{v_1^Tv_2}{v_1^Tv_1} v_1\right) \\
		   &= A v_2 - \frac{v_1^Tv_2}{v_1^Tv_1} A v_1 \\
		   &= \lambda v_2- \frac{v_1^Tv_2}{v_1^Tv_1} \lambda v_1 \\
		   &= \lambda \left( v_2 - \frac{v_1^Tv_2}{v_1^Tv_1} v_1 \right) = \lambda u
		\end{align*}
		and is perpendicular to $v_1$, since
			\begin{align*}
			v_1^T u &= v_1^T\left(v_2 - \frac{v_1^Tv_2}{v_1^Tv_1}v_1\right) \\
					&= v_1^T v_2 - \left(\frac{v_1^Tv_2}{v_1^Tv_1}\right) v_1^T v_1 \\
					&= v_1^T v_2 - v_1^Tv_2 (1) = 0.
			\end{align*}
	\end{proof}
	
	Thus we have that the principal directions form an orthonormal frame at all points $(x,y)$ in the continuous image.
	\subsection{Calculating Derivatives of Discrete Images} 
	proof that derivatives should be taken on gaussian blur.
	\section{Frangi Filter}
	\subsection{Intro to Hessian-based filters}
	Basic properties and observations. Why these filters make sense. Basic strengths and weaknesses.
	\subsection{Overview of Frangi vesselness measure} keep it simple. this maybe should just go in the next chapter
	\subsection{Implementation Details: Convolution Speedup via FFT}
	As described above, the actual computation of derivatives is achieved via convolution with a gaussian. In practice, this is very slow for large scales. In 
		\subsubsection{TODO}
		\begin{itemize}
			\item find image processing papers that find hessian from FFT / who uses this?
			\item with above: downsides?
			\item SIDE BY SIDE comparison?
		\end{itemize}
		
		\subsubsection{2D Discrete Fourier Transform Convolution Theorem}\footnote{the following was adapted in a large part from DFT: an owner's manual. cite?}
		
		\begin{theorem}[2D DFT Convolution Theorem]
		Given two discrete functions are sequences with the same length\footnote{If they're
			not actually the same length, DIP-GW suggests to make the final length at least
			$P = A+C-1$ and $Q = B+D-1$ in the case that the sizes are $A\times B$ and $C\times D$ for $f(x,y)$ and  $h(x,y)$ respectively. Not sure if that matters.}, that is:
		$f(x,y)$ and $h(x,y)$ for integers $0 < x < M$ and $0 < y < N$, we can take the discrete fourier transform (DFT) of each:
		\begin{align}
		F(u,v) := \mathcal{D}\{f(x,y)\} &=
						\sum_{x=0}^{M-1} \sum_{y=0}^{N-1} f(x,y)
						e^{-2\pi i \left(\frac{ux}{M} + \frac{vy}{N}\right)} \\	
		   H(u,v) := \mathcal{D}\{h(x,y)\} &=
						\sum_{x=0}^{M-1} \sum_{y=0}^{N-1} h(x,y)
						e^{-2\pi i \left(\frac{ux}{M} + \frac{vy}{N}\right)}
		\end{align}
		
		and given the convolution of the two functions
		\begin{equation}
		\left(f \star h\right)(x,y) = \sum_{m=0}^{M-1} \sum_{n=0}^{N-1} f(m,n)h(x-m,y-n)
		\end{equation}
		
		then $\left(f \star h\right)(x,y)$ and $MN\cdot F(u,v)H(u,v)$ are transform pairs, i.e.
		\begin{equation}
		\left(f \star h\right)(x,y) = \mathcal{D}^{-1}\left\{MN\cdot F(u,v)H(u,v)\right\}
		\end{equation}
		\end{theorem}
		The proof follows from the definition of convolution, substituting in the inverse-DFT of $f$ and $h$, and then rearrangement of finite sums.
		\begin{proof}
		\begin{align}
		\left(f \star h\right)(x,y) &= \sum_{m=0}^{M-1} \sum_{n=0}^{N-1} f(m,n)h(x-m,y-n) \\
		&= \sum_{m=0}^{M-1} \sum_{n=0}^{N-1}
		\left(\sum_{p=0}^{M-1} \sum_{q=0}^{N-1} F(p,q)
			e^{2\pi i \left(\frac{mp}{M} + \frac{nq}{N}\right)}\right)
			\left(\sum_{u=0}^{M-1} \sum_{v=0}^{N-1} H(u,v)
			e^{2\pi i \left(\frac{u(x-m)}{M} + \frac{v(y-n)}{N}\right)} \right) \\
		&= \left(\sum_{u=0}^{M-1} \sum_{v=0}^{N-1} H(u,v)
			e^{2\pi i \left(\frac{ux}{M} + \frac{vy}{N}\right)}\right)
			\left(\sum_{p=0}^{M-1} \sum_{q=0}^{N-1} F(p,q)
			\left(\sum_{m=0}^{M-1} e^{2\pi i \left(\frac{m(p-u)}{M}\right)}\right)
			\left(\sum_{n=0}^{N-1} e^{2\pi i \left(\frac{n(q-v)}{N}\right)}\right)\right) \\
			&= \left(\sum_{u=0}^{M-1} \sum_{v=0}^{N-1} H(u,v)
			e^{2\pi i \left(\frac{ux}{M} + \frac{vy}{N}\right)}\right)
			\left(\sum_{p=0}^{M-1} \sum_{q=0}^{N-1} F(p,q)
			\left( M \cdot \hat{\delta}_M(p-u) \right)
			\left( N \cdot \hat{\delta}_M(q-v)\right)\right) \\
			&= \left(\sum_{u=0}^{M-1} \sum_{v=0}^{N-1} H(u,v)
			e^{2\pi i \left(\frac{ux}{M} + \frac{vy}{N}\right)}\right)
			\cdot M N F(u,v) \\
			&=MN \cdot \sum_{u=0}^{M-1} \sum_{v=0}^{N-1} F(u,v) H(u,v)
			e^{2\pi i \left(\frac{ux}{M} + \frac{vy}{N}\right)} \\
			&= MN \cdot \mathcal{D}^{-1}\left\{ FH\right\}
		\end{align}
		
		where
		\begin{equation}
			\hat{\delta}_N (k) = \begin{cases}
				1 & \text{when } k = 0 \mod N \\
				0 & \text{else}
				\end{cases}
		\end{equation}
	\end{proof}
	Above, we make use of the following lemma:
	\begin{lemma}
	Let $j$ and $k$ be integers and let $N$ be a positive integer. Then
	\begin{equation}
		\sum_{n=0}^{N-1} e^{2\pi i\left(\frac{n(j-k)}{N}\right)} =  N \cdot \hat{\delta}_N(j-k)
		\end{equation}
		\end{lemma}
		\begin{proof}
		
		For any particular $n \in 0..N-1$,consider the complex number $e^{2\pi i n(j-k)/N}$. Note first, that this is an $N$-th root of unity, since
		\[
		\left(e^{2\pi i n(j-k)/N}\right)^N = e^{2\pi i n(j-k)} = \left(e^{2\pi i}\right)^{n(j-k)}
		= 1^{n(j-k)} = 1
		\]
	
	Thus, we understand that the complex number $e^{2\pi i n(j-k)/N}$ is a root of $z^N -1 = 0$, which we rewrite as
	\[ z^N -1 \;=\; (z-1)(z^{n-1} + \cdots + z + 1) \;=\; (z-1)\sum_{n=0}^{N-1} z^n .
	\]

We consider two cases: in the case that $j-k$ is a multiple of $N$, we of course have $e^{2\pi i n(j-k)/N} = 1$ for any $n$, and thus
\[
\sum_{n=0}^{N-1} e^{2\pi i\left(\frac{n(j-k)}{N}\right)} = \sum_{n=0}^{N-1} \left(1\right) = N
\].

In the case that $j-k$ is \textit{not} a multiple of $N$, then clearly $\left(e^{2\pi i n(j-k)/N}\right)-1 \ne 0$, and thus it must be that
\[\left(e^{2\pi i n(j-k)/N}\right)^N -1 = 0 \;\implies\; \sum_{n=0}^{N-1} \left(e^{2\pi i n(j-k)/N}\right)^n = 0\]
	
	Combining these two cases gives the result of the lemma.
		\end{proof}
%		\subsubsection{Terminology}
%		% this was probably referenced earlier. make it agree and reference earlier eq. number?
%		Call the original (theoretical, nondiscrete) image $L(x,y)$.
%		Denote the FFT'd image $\mathcal{L}(\mu, \nu) := \mathcal{F}\left\{L(x,y)\right\}$.
%		% Is this common? Check.
%		We will also refer to the $\mathcal{L}(\mu,\nu)$ as the image in frequency space.
%		% This was probably referenced earlier. make it agree and reference earlier eq. number?
%		The (theoretical Hessian) of image $L$ is denoted as
%			$H(x,y)$ := $\begin{Bmatrix} L_{xx} & L_{xy} \\ L_{yx} & L_{yy} \end{Bmatrix}$
%		
%		According to \cite{gonzalezwoods} %pp255 table 4.3,
%		we can take the DFT %FUCK, need difference between DFT and FFT.
		
		%%%
		
		
		\subsubsection{FFT}
		
		As noted, the above result applies to the Discrete Fourier Transform. As noted, we actually achieve a convolution speedup using a Fast Fourier Transform (FFT) instead.
		
		Should  I put theory in on this? Yuck.
		
		\subsubsection{Comparision--Notes on Speedup}
		
		Here you actually provide some results that this method is faster. Times, compatibililty of results, etc.
		
		\subsubsection{Sampling Theorem}
		I actually don't know.
		WRAPAROUND ERROR (like from p250) i don't get how that works
\section{Linear Scale Space Theory}
	Koenderink showed that "any image can be embedded in a one-parameter family of derived images (with resolution as the parameter) in essentially only one unique way" given a few of the so-called \textit{scale space axioms}. They showed in particular that any such family must satisfy the heat equation
	\begin{equation}
		\Delta K(x,y,\sigma) = K_\sigma (x,y,\sigma) 
		\;\text{for}\; \sigma \ge 0
		\;\text{such that}\; K(x,y, 0) = u_0(x,y).
		\end{equation}
		where $K: \R^3 \to \R $ and  $u_0: \R^2 \to \R: $ is the original image (viewed as a continuous surface) and $\sigma$ is a resolution parameter.
	\subsection{Overview of Theory}
	\subsection{methods of "merging"}
\section{Other Odds and Ends} Finding the placental plate? Morphology stuff like skeletonization.



